<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hyemingu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hyemingu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-05T04:29:01+00:00</updated><id>https://hyemingu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Lipschitz Regularized Gradient Flows and Latent Generative Particles</title><link href="https://hyemingu.github.io/blog/2023/ICERM_poster_GPA/" rel="alternate" type="text/html" title="Lipschitz Regularized Gradient Flows and Latent Generative Particles"/><published>2023-05-31T17:39:00+00:00</published><updated>2023-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2023/ICERM_poster_GPA</id><content type="html" xml:base="https://hyemingu.github.io/blog/2023/ICERM_poster_GPA/"><![CDATA[<p><strong>Abstract</strong> We constructed gradient flows which minimize Lipschitz regularized f-divergences which are written in variational formulation. Variational formulation enables to approximate a function of likelihood ratio dP/dQ between two empirical distributions obtained from samples. In case of KL-divergence, this function is the log likelihood ratio. We allow flexibility in choosing f depending on the probability distribution to learn, so that heavy-tailed distributions can be fitted using alpha divergences, instead of the KL divergence. On the other hand, Lipschitz regularization leads to the f-divergences bounded even between non-absolutely continuous distributions. In terms of the transport equation of probability distributions in the Wasserstein space, the gradient flow evolves the empirical distribution in direction of the gradients of the function of likelihood ratio that are learned from data. This function is parametrized by neural networks, and its gradients give us the particle dynamics. Hence we transport the particles through the ODEs and generate more samples from the particles trajectory. Moreover, in order to reduce the dimensions, we developed our particle transportation algorithm in latent spaces and applied to high dimensional problems such as image generation and gene expression data merging.</p> <p><a href="/assets/pdf/icerm_2023_poster_hyemin.pdf">View poster</a></p>]]></content><author><name></name></author><category term="generative_modeling"/><category term="gradient_flow"/><category term="particle_transport"/><category term="optimal_transport"/><category term="poseter"/><category term="machine_learning"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a poster for Optimal Transport in Data Science - ICERM 2023.]]></summary></entry><entry><title type="html">Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow</title><link href="https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow/" rel="alternate" type="text/html" title="Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow"/><published>2022-05-31T17:39:00+00:00</published><updated>2022-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow</id><content type="html" xml:base="https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow/"><![CDATA[<p><strong>Abstract</strong> This project introduces an ongoing research to generate samples from a data set where the distribution is unknown. This project keeps focus on mass transportation approach to handle the problem. First, preliminaries on mass transportation problem and gradient flows on probability measures will be briefly introduced. Then, particle descent algorithm which is equipped with a flexible measure of distance will be introduced. The experiments on the low dimensional examples elaborate the dependency of this measure of distance on the target probability distribution. Strengths of this work comes from the flexible choice of the measure of distance and an interpolated behavior between f-divergences and Γ-intergral probability metrics. Also, the efficiency of this algorithm will be seen by comparing the convergence of a different algorithm, generative adversarial network. Then, a different approach fueled by Markov chain monte carlo will be briefly discussed in application of sample generation in a high dimensional data such as image data.</p> <p><a href="/assets/pdf/hyemin_gu_2022s_m697u_project_report.pdf">View paper</a> | <a href="/assets/pdf/hyemin_gu_2022s_math697u_project_presentation.pdf">View slides</a></p>]]></content><author><name></name></author><category term="generative_modeling"/><category term="gradient_flow"/><category term="particle_transport"/><category term="optimal_transport"/><category term="class_project"/><category term="machine_learning"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2022 Spring Project paper/presentation slide for the Stochastic processes class.]]></summary></entry><entry><title type="html">Python ODE solving tutorial</title><link href="https://hyemingu.github.io/blog/2021/python_ode_solving/" rel="alternate" type="text/html" title="Python ODE solving tutorial"/><published>2021-12-05T17:39:00+00:00</published><updated>2021-12-05T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/python_ode_solving</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/python_ode_solving/"><![CDATA[<h2 id="1-python-tutorial-and-jupyternotebook-setup">1. Python Tutorial and JupyterNotebook Setup</h2> <p>If you need an assist to setup your computer to handle numerical computations and showing interactive results, this 15-min video would give you the first step.</p> <p><a href="/assets/pdf/video1-python_tutorial_and_jupyternotebook_setup.pdf">View slides</a> | <a href="https://youtu.be/tHtfhgSVdIY">Youtube video</a></p> <hr/> <h2 id="2python-ode-solver">2.Python ODE solver</h2> <p><a href="/assets/pdf/video2-python_ode_solver.pdf">View slides</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video2-Python_ODE_Solver.ipynb">Jupyter notebook</a> | <a href="https://youtu.be/V2EDJBX4l9w">Youtube video</a></p> <hr/> <h2 id="3-pinn-for-dynamical-systems">3. PINN for Dynamical systems</h2> <p>This 18 min video would provide you the tutorial to “Implementation and Python Library Tutorial for PINNs to Handle Dynamical Systems”.</p> <p>Lorenz model and its inverse/forward problems are chosen as the example for dynamical systems.</p> <p>I attached the modified example code for forward/inverse Lorenz model in jupyter notebook (ipnb) and the slides.</p> <p><a href="/assets/pdf/video3-pinn_for_dynamical_systems.pdf">View slides</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video3-Pytorch-Forward-HarmonicOscillator.ipynb">Jupyter notebook - pytorch PINN</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video3-deepXDE-ForwardInverse-Lorenz.ipynb">Jupyter notebook - DeepXDE</a> | <a href="https://youtu.be/vR5f1gXoVbc">Youtube video</a></p>]]></content><author><name></name></author><category term="dynamical_system"/><category term="teaching"/><category term="machine_learning"/><category term="python_tutorial"/><category term="applied_mathematics"/><category term="tutorial"/><category term="Python"/><summary type="html"><![CDATA[These are 3 days tutorials(videos, slides, jupyter notebooks) for solving ODEs using Python numerical ODE solvers and introducing PINNs. It was a part of 2021 Fall Nonlinear dynamics class. Conducted by T.A. Hyemin Gu.]]></summary></entry><entry><title type="html">Lorenz Equations for Atmospheric Convection Modeling</title><link href="https://hyemingu.github.io/blog/2021/lorenz_eq/" rel="alternate" type="text/html" title="Lorenz Equations for Atmospheric Convection Modeling"/><published>2021-05-31T17:39:00+00:00</published><updated>2021-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/lorenz_eq</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/lorenz_eq/"><![CDATA[<p><strong>Abstract</strong> The Lorenz model is a dynamical system of three first order differential equations. It was designed by Lorenz as a simplified model of atmospheric convection. This project assumes that the Earth’s atmosphere is an incompressible fluid situated between two horizontal planes. Using governing equations in 2D hydrodynamics, the steps of Lorenz are followed to derive the Lorenz equations from an abstract climate model. Then, properties of the Lorenz equations are explored and illustrated by individual examples and their interpretations.</p> <p><a href="/assets/pdf/project_presentation_applied_math_modeling_hyemingu.pdf">View slides</a> | <a href="/assets/pdf/project_paper_applied_math_modeling.pdf">View paper</a></p>]]></content><author><name></name></author><category term="dynamical_system"/><category term="class_project"/><category term="applied_mathematics"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2021 Spring Project paper for the Applied Math and Math Modeling class.]]></summary></entry><entry><title type="html">Learning operators</title><link href="https://hyemingu.github.io/blog/2021/learning_operator/" rel="alternate" type="text/html" title="Learning operators"/><published>2021-05-31T17:39:00+00:00</published><updated>2021-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/learning_operator</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/learning_operator/"><![CDATA[<p><strong>Abstract</strong> As the Neural Network framework(Physics-informed Neural Network) is introduced to solve PDEs and dynamical systems, solutions of differential equations are learned from data, and this could replace or improve the conventional numerical solvers. Still, there are limitations on this “learning functions”. A new framework of “learning operators” came up in order to cure the limitations, and in specific, DeepONet provided a Neural Network architecture for learning operators which map functions to functions. This talk will show the mechanism and deeds of DeepONet(2020) and mention its follow-up studies.</p> <p><a href="/assets/pdf/learning_operators.pdf">View slides</a></p>]]></content><author><name></name></author><category term="machine_learning"/><category term="differential_equations"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a review presentation slide delivered in machine learning reading seminar in 2021.]]></summary></entry><entry><title type="html">Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space</title><link href="https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS/" rel="alternate" type="text/html" title="Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space"/><published>2021-05-30T17:39:00+00:00</published><updated>2021-05-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS/"><![CDATA[<p><strong>Abstract</strong> Support Vector Machine (SVM) provides a linear classifier for binary classification problems. Complex decision boundaries in the input feature space are handled by nonlinear kernels to the SVM. Theories in Reproducing Kernel Hilbert Spaces (RKHS) state that, given a kernel \(\mathcal{K}\) and a set of \(M\) given data \((x_i,y_i)\), for \(i=1,\cdots,M\), a SVM classifier function can be written as \(f(x) =\alpha_0 + \sum_{i=1}^M \alpha_i \mathcal{K}(x, x_i)\) for some coefficients \(\alpha_i\)s. Also, applying conformal transforms to a positive definite kernel produces another positive definite kernel which are in more complexity. Hence, in case that well-known kernels fail given the current training data, a new kernel can be tried by optimizing the coefficients of a conformal kernel in the way to maximize the ratio “(Between-class error)/(Within-class error)” of the training data. Here, data-dependent kernel SVM is applied to an application of classifying tumor/tumor-free organs from gene expression data and compared its classification performance with other well-known kernels.</p> <p><a href="/assets/pdf/project_paper_math_found_of_ai.pdf">View file</a></p>]]></content><author><name></name></author><category term="machine_learning"/><category term="class_project"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2021 Spring Project paper for the ST - Math Foundations of Probabilistic Artificial Intelligence II class.]]></summary></entry><entry><title type="html">R Bioinformatics 2. R Genome informatics methodology and practice.</title><link href="https://hyemingu.github.io/blog/2020/r_bioinformatics/" rel="alternate" type="text/html" title="R Bioinformatics 2. R Genome informatics methodology and practice."/><published>2020-12-30T17:39:00+00:00</published><updated>2020-12-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/r_bioinformatics</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/r_bioinformatics/"><![CDATA[<p>This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.</p> <p><a href="/assets/pdf/r_bioinformatics.pdf">View file</a></p>]]></content><author><name></name></author><category term="teaching"/><category term="r_bioinformatics"/><category term="bioinformatics"/><category term="tutorial"/><category term="R"/><summary type="html"><![CDATA[This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.]]></summary></entry><entry><title type="html">R Bioinformatics 1. R Statistics.</title><link href="https://hyemingu.github.io/blog/2020/r_statistics/" rel="alternate" type="text/html" title="R Bioinformatics 1. R Statistics."/><published>2020-09-30T17:39:00+00:00</published><updated>2020-09-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/r_statistics</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/r_statistics/"><![CDATA[<p>This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.</p> <p><a href="/assets/pdf/r_statistics.pdf">View file</a></p>]]></content><author><name></name></author><category term="teaching"/><category term="r_bioinformatics"/><category term="bioinformatics"/><category term="tutorial"/><category term="R"/><summary type="html"><![CDATA[This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.]]></summary></entry><entry><title type="html">RA ch12 Autocorrelation in time series data</title><link href="https://hyemingu.github.io/blog/2020/regression_ch12/" rel="alternate" type="text/html" title="RA ch12 Autocorrelation in time series data"/><published>2020-07-11T15:12:00+00:00</published><updated>2020-07-11T15:12:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/regression_ch12</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/regression_ch12/"><![CDATA[<h1 id="ch-12-autocorrelation-in-time-series-data">Ch 12: Autocorrelation in time series data</h1> <p>In the previous chapters, errors \(\epsilon_i\)’s are assumed to be</p> <ul> <li>uncorrelated random variables or</li> <li>independent normal random variables.</li> </ul> <p>However, in business and economics, time series data often fail to satisfy above assumption. In time series data, error terms are likely to be autocorrelated / serially correlated over time. Is is mainly because</p> <ul> <li>one or more important predictor variables are missing (when time-ordered effects of certain variables are positively correlated, so do error terms in the regression model. The error terms would include effects of missing variables.)</li> <li>there are systematic coverage errors in the response variable.</li> </ul> <p>Autocorrelation or positively autocorrelated error terms may trigger problems such as:</p> <ul> <li>\(E(b) = \beta\) but \(Var(b)\) is not the minimum</li> <li>[\(MSE\) / \(s(b_k)\) ] may seriously underestimate [\(\sigma^2\) / \(\sigma(b_k)\)] \(\Rightarrow\) confidence intervals and tests are no longer applicable.</li> <li>the regression line sharply differs from the true line</li> <li>the regression line is sensitive to the initial error \(\epsilon_0\)</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/problem_of_autocorrelation-480.webp 480w,/assets/img/regression/problem_of_autocorrelation-800.webp 800w,/assets/img/regression/problem_of_autocorrelation-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/regression/problem_of_autocorrelation.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Problem of autocorrelation </div> <p>Therefore, we need to analyze a regression model with autoregressive errors, test if a model is autocorrelated and then remedy autocorrelation.</p> <h2 id="outline">Outline</h2> <ol> <li> <p>First order autoregressive error model</p> <ul> <li>One predictor variable case</li> <li>properties of error terms</li> </ul> </li> <li>Durbin-Watson test for autocorrelation</li> <li>Remedial measures for autocorrelation <ul> <li>addition of predictor variables</li> <li>use of transformed variables <ul> <li>way 1: Cochrane-Orcutt procedure</li> <li>way 2: Hildreth-Lu procedure</li> <li>way 3: first difference procedure</li> </ul> </li> </ul> </li> <li>forecasting with autocorrelated error terms</li> </ol> <hr/> <h2 id="1-first-order-autoregressive-error-model">1. First order autoregressive error model</h2> <h3 id="one-predictor-variable-case">one predictor variable case</h3> <p>The generalized simple linear regression model for one predictor variable when the random error terms follow a <strong>first order autoregressive, or AR(1), process</strong> is</p> <p>1: \(Y_t = \beta_0 + \beta_1 X_t + \epsilon_t\)</p> <p>2: \(\epsilon_t = \rho \epsilon_{t-1} + u_t\)</p> <p>where \(\rho\) is a parameter such that \(|\rho| &lt;1\) and \(u_t\) are independent \(N(0, \sigma^2)\). \(\rho\) is called the <strong>autocorrelation parameter</strong>.</p> <p>Above one variable case can be generalized to multiple variables case. The only thing changed is the first line 1’: \(Y_t = \beta_0 + \beta_1 X_{t1} + \cdots + \beta_{p-1} X_{t,p-1} +\epsilon_t\).</p> <p><u>Remark</u> When it comes to a second-order process, the line 2 is changed to 2’: \(\epsilon_t = \rho_1 \epsilon_{t-1} + \rho_2 \epsilon_{t-2} + u_t\).</p> <h3 id="properties-of-error-terms">properties of error terms</h3> <ol> <li> \[E(\epsilon_t) = 0\] </li> <li> \[\sigma^2(\epsilon_t) = \frac{\sigma^2}{1-\rho^2}\] </li> <li> \[\sigma(\epsilon_t, \epsilon_{t-1}) = \rho \frac{\sigma^2}{1-\rho^2}\] </li> <li> \[\sigma(\epsilon_t, \epsilon_{t-s}) = \rho^s \frac{\sigma^2}{1-\rho^2}\] </li> <li> \[\rho(\epsilon_t, \epsilon_{t-1}) = \frac{\sigma(\epsilon_t, \epsilon_{t-1})}{\sigma(\epsilon_t) \sigma(\epsilon_{t-1})} = \rho\] </li> <li> \[\rho(\epsilon_t, \epsilon_{t-s}) = \frac{\sigma(\epsilon_t, \epsilon_{t-1})}{\sigma(\epsilon_t) \sigma(\epsilon_{t-1})} = \rho^s\] </li> <li>\(\rho=0\) implies that the error terms are uncorrelated. <blockquote> <p><u>Proof</u> Note that \(\epsilon_t = \rho \epsilon_{t-1} + u_t = \rho^2 \epsilon_{t-2} + \rho u_{t-1} + u_t\) &gt; \(= \rho^3 \epsilon_{t-3} + \rho^2 u_{t-2} + \rho u_{t-1} + u_t = \cdots =\) <strong>\(\sum_{s=0}^\infty \rho^s u_{t-s}\)</strong>.</p> <p>(2) Since \(u_t\)’s are uncorrelated to each other and \(\sigma^2(u_t) = \sigma^2\), \(\sigma^2(\epsilon_t) = \sum_{s=0}^\infty \rho^{2s} \sigma^2(u_{t-s}) = \sigma^2 \sum_{s=0}^\infty \rho^{2s} = \sigma^2 \frac{1}{1-\rho^2}\) (\(|\rho|&lt;1\)).</p> <p>(3) Since \(E(\epsilon_t) = 0\), \(\sigma(\epsilon_t, \epsilon_{t-1}) = E(\epsilon_t, \epsilon_{t-1})\) &gt; \(=E[(u_t + \rho u_{t-1} + \rho^2 u_{t-2} + \cdots )(u_{t-1} + \rho u_{t-2} + \rho^2 u_{t-3} + \cdots ) ]\) &gt; \(=E[u_t + \rho (u_{t-1} + \rho u_{t-2} + \cdots )(u_{t-1} + \rho u_{t-2} + \rho^2 u_{t-3} + \cdots ) ]\) &gt; \(= E[u_t (u_{t-1} + \rho u_{t-2} + \cdots )] + \rho E [ u_{t-1} + \rho u_{t-2} + \cdots ]^2\). By independency of \(u_t\), \(E(u_t u_{t-s}) E(u_t) E(u_{t-s})= 0 ~\forall s \neq 0\). \(\Rightarrow \sigma(\epsilon_t, \epsilon_{t-1}) = \rho E [ u_{t-1} + \rho u_{t-2} + \cdots ]^2 = \rho \sigma^2(\epsilon_{t-1}) = \rho \frac{\sigma^2}{1-\rho^2}\).</p> </blockquote> </li> </ol> <p>\(n \times n\) variance-covariance matrix of error \(\sigma^2(\epsilon)\) is given as \([ \kappa, \kappa \rho, \kappa \rho^2, \cdots, \kappa \rho^{n-1} ; \kappa \rho, \kappa, \kappa \rho, \cdots, \kappa \rho^{n-2} ; \vdots ; \kappa \rho^{n-1}, \kappa \rho^{n-2}, \kappa \rho^{n-3}, \cdots, \kappa ]\) where \(\kappa = \frac{\sigma^2}{1-\rho^2}\).</p> <h2 id="2-durbin-watson-test-for-autocorrelation">2. Durbin-Watson test for autocorrelation</h2> <p>Now, we want to determine if the regression model has autocorrelation or not. Note that \(\rho=0\) implies \(\epsilon_t = u_t\) ; uncorrelated.</p> <p>\(H_0 : \rho=0\) \(H_1 : \rho&gt;0\) (we consider the positive autocorrelation.)</p> <p>Test statistic</p> \[D = \frac{\sum_{t=2}^n (e_t - e_{t-1})^2}{\sum_{t=1}^n e_t^2}\] <p>where \(e_t = Y_t - \hat{Y_t}\).</p> <p>Exact critical values are difficult to obtain, but there is an upperbound \(d_u\) and lowerbound \(d_l\) such that \(D\) outside these bounds leads to a definite decision. Decision rule:</p> <ul> <li>If \(D &gt; d_u\), conclude \(H_0\).</li> <li>If \(D &lt;\), conclude \(H_1\).</li> <li>If \(d_l &lt; D &lt; d_u\), inconclusive.</li> </ul> <p>idea: Since \((\epsilon_t - \epsilon_{t-1})^2\) is small when the errors are positively autocorrelated, small \(D\) implies \(\rho &gt;0\).</p> <h2 id="3-remedial-measures-for-autocorrelation">3. Remedial measures for autocorrelation</h2> <h3 id="addition-of-predictor-variables">addition of predictor variables</h3> <p>Time-ordered effects are often due to the missing key variables. When the long-term persistent effects cannot be captured by one or several predictor variables, a <strong>trend component</strong> can be added to the regression model, such as a linear/exponential trend, or indicator variables for seasonal effects.</p> <h3 id="use-of-transformed-variables">use of transformed variables</h3> <p>When it failed to cure autocorrelation from addition of predictor variables, we may consider transforming variables to obtain an ordinary regression model. It can be done by \(Y_t^\prime = Y_t - \rho Y_{t-1}\)</p> \[\Rightarrow Y_t^\prime = (\beta_0 + \beta_1 X_t + \epsilon_t) - \rho (\beta_0 + \beta_1 X_{t-1} + \epsilon_{t-1}) = \beta_0 (1 - \rho) + \beta_1 (X_t - \rho X_{t-1}) + u_t\] <p>Set \(X_t^\prime = X_t - \rho X_{t-1}\), \(\beta_0^\prime = \beta_0 (1 - \rho)\), and \(\beta_1^\prime = \beta_1\). \(\Rightarrow\) <strong>\(Y_t^\prime = \beta_0^\prime + \beta_1^\prime X_t^\prime + u_t\)</strong> where \(u_t\)’s are uncorrelated and follows \(N(0,\sigma^2)\).</p> <p>To use the transformed model, we need to estimate \(\rho\). Let’s say that we have \(r\), an estimator of \(\rho\). Then, from the original fitted function \(\hat{Y} = b_0 + b_1 X\), we can restore \(b_0 = \frac{b_0^\prime}{1-r}\), and \(b_1 = b_1^\prime\) where \(s(b_0) = \frac{s(b_0^\prime)}{1-r}\), and \(s(b_1) = s(b_1^\prime)\).</p> <p>There are three different procedures to obtain transformed models.</p> <h6 id="way-1-cochrane-orcutt-procedure">way 1: Cochrane-Orcutt procedure</h6> <p>It consists of three steps and their iterations. Each step is:</p> <ol> <li>Estimate \(\rho\). <br/> Consider a regression function through the origin: \(\epsilon_t = \rho \epsilon_{t-1} + u_t\) where \(u_t\) is a random disturbance term. Since \(\epsilon_t\) and \(\epsilon_{t-1}\) are unknown, we use \(e_t\) and \(e_{t-1}\), instead. The estimator of the slope \(r\) is \(r = \frac{\sum_{t=2}^n e_{t-1}e_t}{\sum_{t=2}^n e_{t-1}^2}\)</li> <li>Fit a transformed model. \(Y_t^\prime = \beta_0^\prime + \beta_1^\prime X_t^\prime + u_t\)</li> <li>Test for autocorrelation. By Durbin-Watson’s test</li> </ol> <p><u>Note</u> This approach does not always work properly. When the error terms are positively autocorrelated, the estimate \(r\) tends to underestimate \(\rho\).</p> <h6 id="way-2-hildreth-lu-procedure">way 2: Hildreth-Lu procedure</h6> <p>This is analogous to the Box-Cox procedure. The value of \(\rho\) is chosen to minimize the error sum of squres for the <strong>transformed</strong> regression model. \(SSE = \sum_t (Y_t^\prime - \hat{Y_t^\prime})^2 = \sum_t (Y_t^\prime - b_0^\prime - b_1^\prime X_t^\prime)^2\) Then test for autocorrelation (By Durbin-Watson’s test).</p> <p><u>Note</u> This approach does not need iterations. But optimization process is required.</p> <h6 id="way-3-first-difference-procedure">way 3: first difference procedure</h6> <p><u>Facts</u></p> <ul> <li>The autocorrelation parameter \(\rho\) is frequently large (close to 1).</li> <li>\(SSE(\rho)\) is often quite flat for large values of \(\rho\). (When using optimization approach (way 2: Hildreth-Lu procedure).</li> </ul> <p>Therefore, <strong>take \(\rho = 1\)</strong>. Then, \(\beta_0^\prime = \beta_0 (1-\rho) = 0\) and the transformed model is <strong>\(Y_t^\prime = \beta_1^\prime X_t^\prime + u_t\)</strong> where \(Y_t^\prime = Y_t - \rho Y_{t-1}\), and \(X_t^\prime = X_t - \rho X_{t-1}\). In the transformed model, the intercept term is canceled out.</p> <p>The fitted regression line in the transformed variables is \(\hat{Y^\prime} = b_1^\prime X^\prime\) which can be transformed back to the original variables as \(\hat{Y} = b_0 + b_1 X\) where \(b_0 = \bar{Y} - b_1^\prime \bar{X}\), and \(b_1 = b_1^\prime\).</p> <p><u>Note</u> This approach is effective in a variety of applications and much simpler than other 2 procedures. (The estimation process of \(\rho\) is deleted.)</p> <h2 id="4-forecasting-with-autocorrelated-error-terms">4. forecasting with autocorrelated error terms</h2> <p>\(Y_t = \beta_0 + \beta_1 X_t + \epsilon_t\) where \(\epsilon_t = \rho \epsilon_{t-1} + u_t\). i.e. \(Y_t =\)<strong>\(\beta*0 + \beta_1 X_t\)</strong> \(+\) <strong>\(\rho \epsilon*{t-1}\)</strong> \(+\) **\(u_t\)**.</p> <p>Thus, the forecast for the next period \(t+1\), \(F_{t+1}\) is constructed by handling the three components:</p> <ol> <li>Given \(X_{t+1}\), estimate \(\beta_0 + \beta_1 X_t\) from the fitted regression function \(\hat{Y_{t+1}} = b_0 + b_1 X_{t+1}\).</li> <li>\(\rho\) is estimated by \(r\) and \(\epsilon_t\) is estimated by \(e_t\). i.e. \(e_t = Y_t - (b_0 + b_1 X_t) = Y_t - \hat{Y_t}\). Thus, \(\rho \epsilon_t\) is estiamted by \(re_t\).</li> <li>The disturbance term \(u_{t+1}\) has expected value zero and is independent of earlier information. Therefore \(E(u_{t+1}) = 0\).</li> </ol> <p>\(\Rightarrow\) The forecast for the period \(t+1\) is <strong>\(F_{t+1} = \hat{Y_{t+1}} + re_t\)</strong>.</p> <p>An approximate \(1-\alpha\) prediction interval for \(Y_{t+1, new}\) is obtained by employing <strong>the usual prediction limits for a new observation</strong> but <strong>based on the transformed observation</strong>. The process is \((X_i, Y_i) \rightarrow (X_i^\prime, Y_i^\prime) \rightarrow s^2(pred)\)</p> <p>Therefore, an approximate \(1-\alpha\) prediction interval for \(Y_{t+1, new}\) is \(F_{t+1} \pm t(1-\alpha/2 ; n-3) s(pred)\) Here, the degree of freedom is \(t-3\) because only \(t-1\) transformed cases are used, and 2 degrees are lost for estimating the parameters \(\beta_0\) and \(\beta_1\).</p>]]></content><author><name></name></author><category term="teaching"/><category term="regression_analysis"/><category term="statistics"/><category term="lecture_note"/><summary type="html"><![CDATA[Lecture note chapter 12 for regression analysis taught by Hyemin Gu in 2020]]></summary></entry><entry><title type="html">RA ch11 Remedial measures</title><link href="https://hyemingu.github.io/blog/2020/regression_ch11/" rel="alternate" type="text/html" title="RA ch11 Remedial measures"/><published>2020-07-10T15:12:00+00:00</published><updated>2020-07-10T15:12:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/regression_ch11</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/regression_ch11/"><![CDATA[<h1 id="ch-11-remedial-measures">Ch 11: Remedial measures</h1> <p>Transformation is one of the standard remedial measure for a linear model. Recall that its uses are:</p> <ol> <li>to linearize the regression relationship</li> <li>to make the error distribution more nearly normal</li> <li>to make the variances of the error terms more nearly equal.</li> </ol> <p>In this chapter, we will find additional remedial measures to handle several pitfalls. Then we discuss non-parametric regression methods (which are quite different from previous regression models). A common feature of the remedial measures and alternative regression methods is that estimation procedures from the ways we’ve seen are relatively complex, so we need easier and more generic way to evaluate the precision of these complex estimators. Bootstrapping is one example.</p> <h2 id="outline">Outline</h2> <ol> <li> <p>Other remedial measures</p> <ul> <li>unequal error variance - weighted least squares</li> <li>multicollinearity - ridge regression</li> <li>influential cases - robust regression</li> </ul> </li> <li> <p>Nonparametric regression</p> <ul> <li>regression trees</li> </ul> </li> <li> <p>Bootstrap confidence intervals</p> </li> </ol> <hr/> <h2 id="1-other-remedial-measures">1. Other remedial measures</h2> <h3 id="unequal-error-variance---weighted-least-squares">unequal error variance - weighted least squares</h3> <p><strong>Generalized multiple regression model</strong> \(Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i\) where \(e_i\)’s are independent, \(e_i \sim N(0, \sigma_i^2)\), $$i = 1,2,\cdots, n</p> <p>\(. Here, the error variances may not be equal, so that\)\sigma^2(\epsilon)\(is an\)n\times n\(diagonal matrix where each entry has\)\sigma_i^2$$.</p> <p>When we use ordinary least square estimators, then still we have these properties:</p> <ol> <li>the estimators of \(\beta\) are unbiased</li> <li>the estimators are <em>consistent</em> <em>consistency</em> is defined that \(\forall \varepsilon &gt;0, P(|b_n - \beta| \leq \varepsilon) = 1\) for succiciently large \(n\) (\(b_n\) converges to \(\beta\)). However, the estimators are no longer have minimum variance. This is due to the unequal error variances, making \(n\) cases no longer have the same reliability. In other words, observations with small variances provide more reliable information.</li> </ol> <p>A remedy to handle this problem is to use <strong>weighted least squares</strong>. First, we start from the simplest case.</p> <p><strong>when error variances are known</strong> Suppose that the errror variances \(\sigma_i^2\) for all \(n\) observations are given. Then we use <strong>Maximum likelihood method</strong>, again. First, we define the likelihood function \(L(\beta)\).</p> \[L(\beta) = \prod_{i=1}^n \frac{1}{(2\pi \sigma_i^2)^\frac{1}{2}} exp [ - \frac{1}{2 \sigma_i^2} (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2 ]\] <p>Here, we substitute \(w_i = \frac{1}{\sigma_i^2}\) and rewrite the formula.</p> \[L(\beta) = \prod_{i=1}^n \left( \frac{w_i}{2\pi}\right)^\frac{1}{2} exp [ - \frac{w_i}{2} (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2 ]\] <p>Since \(\sigma_i^2\)’s are known, parameters for likelihood function are just regression coefficients. Maximum likelihood method finds critical points which maximizes the likelihood function value.</p> <p>This is just as the same as minimizing \(Q_w = \sum_{i=1}^n w_i (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2\). The normal equation can be written using weights matrix \(W = diag(w_1, w_2, \cdots, w_n)\) as below:</p> \[X^t WX b_w = X^tW Y\] <p>\(\Rightarrow b_w = (X^t WX)^{-1} X^tW Y\) where \(\sigma^2(b_w) = (X^t WX)^{-1}\). This is called <strong>Weighted least square estimator</strong>.</p> <p><strong>when variances are known up to proportionality constant</strong> Now, relaxing the condition, assume that we only know proportions among error variances. Consider a case that the second error variance is twice larger than the first one. i.e. \(\sigma_2^2 = 2*\sigma_1^2\). Then we can write \(w_i\)’s in this form: \(w_i = \frac{1}{k_i\sigma^2}\) where \(\sigma^2\) is unknown true error variance of \(\epsilon_1\). Then, \(p \times p\) matrix \(\sigma^2 (b_w) = \sigma^2 (X^t K X)^{-1}\), where \(K = diag(k_1, k_2, \cdots, k_n)\). Since \(\sigma^2\) is unknown, we use \(s^(b_w) = MSE_w (X^t K X)^{-1}\), instead, where \(MSE_w = \frac{\sum_{i=1}^n w_i (Y_i - \hat{Y_i})^2}{n-p} = \frac{\sum_{i=1}^n w_i e_i^2}{n-p}\).</p> <p><strong>when error variances are unknown</strong> We use estimators of the error variances. There are two approaches.</p> <ol> <li>Estimation by residuals \(\sigma_i^2 = E(\epsilon^2) - [E(\epsilon)]^2 = E(\epsilon^2)\). Hence, the squared residual \(e_i^2\) is an estimator of \(\sigma_i^2\).</li> <li>Use of replicates or near replicates <ul> <li>In a designed experiment (such as scientific simulations), replicate observations could be made. If the number of replicates is large, use the fact that sample variance gets closer to variance.</li> <li>However, in an observational studies (such as social studies), we use near observations as replicates.</li> </ul> </li> </ol> <p>Inference procedures should be followed when weights are estimated. We want to estimate a variance-covariance matrix \(\sigma^2(b_w)\). The confidence intervals for regression coefficients are estimated with \(s^2(b_{w_k})\) where \(k\) stands for a bootstrap index. Then apply bootstraping method.</p> <p>Lastly, we can use the ordinary least squares leaving the error variances unequal. Regression coefficient \(b\) is still unbiased and consistent, but no longer a minimum variance estimator, where its variance is given as \(\sigma^2(b) = (X^tX)^{-1} X^t \sigma^2(\epsilon)X (X^tX)^{-1}\) and it is estimated by \(s^2(b) = (X^tX)^{-1} X^t S_0 X (X^tX)^{-1}\) where \(S_0 = diag(e_1^2, e_2^2, \cdots, e_n^2)\).</p> <p><u>Remark on weighted least squares</u> Weighted least squares can be interpreted as transforming the data \((X_i, Y_i, \epsilon_i)\) by \(W^{\frac{1}{2}}\). This can be written as</p> \[W^{\frac{1}{2}}Y = W^{\frac{1}{2}}X\beta + W^{\frac{1}{2}}\epsilon\] <p>and by setting \(Y_w = W^{\frac{1}{2}} Y\), \(X_w = W^{\frac{1}{2}} X\), and \(\epsilon_w = W^{\frac{1}{2}} \epsilon\), the transformed variables follow the ordinary linear regression model</p> \[Y_w = X_w \beta + \epsilon_w\] <p>where \(E(\epsilon_w) = W^{\frac{1}{2}} E(\epsilon) = W^{\frac{1}{2}} 0 = 0\) and \(\sigma^2(\epsilon_w) = W^{\frac{1}{2}} \sigma^2(\epsilon) W^{\frac{1}{2}} = W^{\frac{1}{2}} W^{-1} W^{\frac{1}{2}} = I\). The regression coefficients are given as \(b_w = (X_w^t X_w)^{-1} X_w^t Y_w\).</p> <h3 id="multicollinearity---ridge-regression">multicollinearity - ridge regression</h3> <p>When multicollineariry is detected in a regression model, we have fixed the model by</p> <ol> <li>using centered data in polynomial regression models</li> <li>drop some redundant predictor variables</li> <li>add some observations that could break the multicollinearity</li> <li>use pricipal components instead of the current variables \(X_k\)</li> <li><strong>Ridge regression</strong>.</li> </ol> <p>Ridge regression perturb the estimated coefficient \(b_R\) from the unbiased estimator \(b\), and therefore it could remedy multicollinearity problem. In other words, our unbiased estimator \(b\) is obtained by the observation \(X\) with multicollinearity, but slightly perturbed estimator \(b_R\) is obtained by a virtual perturbed observation \(X_R\) without multicollinearity. When perturbing \(b_R\), we add restriction on \(b_R\) to have small norm(length). Length restriction makes the variance of the sampling distribution of \(b_R\) shrink. Although, \(b_R\) is no longer an unbiased estimator, we have more chance to obtain \(E(b_R)\) from a sampled \(b_R\) compared to obtaining \(E(b)=\beta\) from a sampled \(b\). The relationship is given as a figure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/ridge_regression-480.webp 480w,/assets/img/regression/ridge_regression-800.webp 800w,/assets/img/regression/ridge_regression-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/regression/ridge_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ridge regression </div> <p>To obtain \(b_R\), we consider using correlation transformed \(X\) observations. This process normalizes the variable \(X\) adjusting the magnitude of \(SSE = (Y-X\beta)^t (Y-X\beta)\). \(b_R\) can be obtained by minimizing a modified sum suquared error function \(Q(\beta) = (Y-X\beta)^t (Y-X\beta) + c \beta^t \beta\) where \(c\) is a given biasing coefficient and \(c \leq 0\). Squared \(\ell_2\) norm of \(\beta\) is added in the function. This allows to minimize both \(SSE\) and the length of \(\beta\), at the same time. The intensity to keep focus on minimizing the length of \(\beta\) can be adjusted by choosing the coefficient \(c\). Larger the value \(c\), the minimizing process would be more focused on minimizing the length of \(\beta\). Resulting normal equation is \((X^t X + cI) b_R = X^t Y\). Therefore, \(b_R = (X^tX + cI)^{-1} X^tY\).</p> <p><strong>Choice of biasing constant \(c\)</strong> As the biasing constant \(c\) increases, the bias of the model would be increased. But there exists a value of \(c\) for which the regression estimator \(b_R\) has a smaller total mean squared error \(E(b^R - \beta)^2\) than the ordinary LSE \(b\). But optimum value of \(c\) varies from one application to another and it is unknown. Common heuristics to determine an appropriate \(c\) are:</p> <ol> <li>using Ridge trace Try several \(c\) and plot \(b_R\)’s according to values of \(0 \leq c \leq 1\). Take the point \(c^\ast\) where the fluctuation in \(b_R\) ceases.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/ridge_trace-480.webp 480w,/assets/img/regression/ridge_trace-800.webp 800w,/assets/img/regression/ridge_trace-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/regression/ridge_trace.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> ridge trace </div> <ol> <li>using \(VIF_k\) Try several \(c\) and plot \(VIF_k\)’s according to values of \(0 \leq c \leq 1\). Take the point \(c^\ast\) where the fluctuation in \(b_R\) ceases.</li> </ol> <h3 id="influential-cases---robust-regression">influential cases - robust regression</h3> <p>When an outlying influential case is not clearly erroneous, we should proceed a further examination to obtain important clues to improve the model. We may find</p> <ul> <li>the omission of an important predictor variable,</li> <li>incorrect functional forms, or</li> <li>needs to dampen the influence of such cases.</li> </ul> <p><strong>Robust regression</strong> is an approach of regression to make the results less rely on outlying cases. There are several ways to conduct robust regression:</p> <ol> <li>LAR (= LAD = minimum \(\ell_1\)-error) regression Least Absolute Residuals (= Least Absolute Deviations = minimum ell-one-error regression) is done by minimizing \(Q_1(\beta) = \sum_{i=1}^n | Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1} |\). When there’s an outlier where its residual is large \(e_i &gt; 1\), then its squared residual gets much larger so that the regression function could overweigh this observation. This can be alleviated by using absolute values of residuals. Note that \(\ell_1\) function is not differentiable. Thus, we cannot obtain an estimator \(\hat{b}\) using partial derivatives. Instead, this problem can be dealt by linear programming (a kind of optimization technique).</li> <li>IRLS robust regression Iteratively Reweighted Least Squares. This approach uses the weighted least square procedure as introduced before, but, weights on residuals are now determined by the magnitude of how far outlying a case is. The weights are updated iteratively in a way to flatten the weights.</li> <li>LMS regression Least Median Squares. Recall that average is sensitive to outliers, but median is not. Thus, we could minimize the function \(median (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2\).</li> </ol> <h2 id="2-nonparametric-regression">2. Nonparametric regression</h2> <p>For complex cases, it is hard to guess a right functional form (analytic expression) of the regression function. Also, as the number of parameter increases, the space of observations gets rarefied (there are fewer cases in a neighborhood), so that the regression function tends to behave erratically. For these cases, nonparametric regression could be helpful.</p> <h3 id="regression-trees">regression trees</h3> <p>Regression tree is a powerful, yet computationally simple method of nonparametric regression. It requires virtually no assumptions, and it is simple to interpret. It is popular for explanatory studies, especially for large data sets.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/regression_tree-480.webp 480w,/assets/img/regression/regression_tree-800.webp 800w,/assets/img/regression/regression_tree-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/regression/regression_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> regression tree </div> <p>In each branch of a regression tree, the range of a specific predictor variable \(X_k\) is partitioned into segments (one for \(X_k \leq C\) and the other for \(X_k &lt; C\)). In the terminal nodes of the tree, we evaluate the estimated regression fit by taking the mean value of the response in that segment. i.e. \(\hat{Y_i} = \sum_{j=1}^{n_s} Y_j\) where \(Y_j\)’s belong to a segment where \(i\)th observation belongs to. Also, \(n_s\) is the number of observations in the segment.</p> <p>How to find a “best” regression tree is linked with</p> <ul> <li>the number of segmented regions \(r\), and</li> <li>the split points \(X_k\) that optimally divides the data into two sets at each branch.</li> </ul> <p>The basic idea is to choose a model which minimizes the error sum of squres for the resulting regression tree. \(SSE = \sum_{t_i \in T} SSE(t_i)\), where \(T\) is the set of all terminal nodes, and \(SSE(t_i) = \sum_{j=1}^n (Y_j - \overline{Y_{t_i}})^2\). Here, \(MSE = SSE/(n-r)\)) and \(R^2 = 1- \frac{SSE}{SSTO}\). The number of candidate models are \(r(p-1)\) where \(1 \leq r \leq n\). The value of \(r\) is usually chosen through validation studies as \(argmin_r{MSPR}\).</p> <h2 id="3-bootstrap-confidence-intervals">3. Bootstrap confidence intervals</h2> <p>Above regression approaches are complex so that it is hard to estimate confidence intervals using the previous analysis on \(\sigma^2(b)\) or \(s^2(b)\). Instead, we can approximate confidence intervals by <strong>bootstrap method</strong>. There are many different procedures to gain bootstrap confidence intervals. One example is <em>reflection method</em>.</p> <p>An \(1-\alpha\) confidence interval of a parameter \(\beta\) is \(b - (b^\ast(1-\alpha/2) - b) \leq \beta \leq b + (b - b^\ast(\alpha/2))\) where \(b^\ast(1-\alpha/2)\) and \(b^\ast(\alpha/2)\) are obtained from bootstrap distribution. $$</p>]]></content><author><name></name></author><category term="teaching"/><category term="regression_analysis"/><category term="statistics"/><category term="lecture_note"/><summary type="html"><![CDATA[Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020]]></summary></entry></feed>