<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://hyemingu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://hyemingu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-08-04T21:31:36+00:00</updated><id>https://hyemingu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</title><link href="https://hyemingu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/" rel="alternate" type="text/html" title="Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra"/><published>2024-05-14T00:00:00+00:00</published><updated>2024-05-14T00:00:00+00:00</updated><id>https://hyemingu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra</id><content type="html" xml:base="https://hyemingu.github.io/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[We’re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.]]></summary></entry><entry><title type="html">Lipschitz Regularized Gradient Flows and Latent Generative Particles</title><link href="https://hyemingu.github.io/blog/2023/ICERM_poster_GPA/" rel="alternate" type="text/html" title="Lipschitz Regularized Gradient Flows and Latent Generative Particles"/><published>2023-05-31T17:39:00+00:00</published><updated>2023-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2023/ICERM_poster_GPA</id><content type="html" xml:base="https://hyemingu.github.io/blog/2023/ICERM_poster_GPA/"><![CDATA[<p><strong>Abstract</strong> We constructed gradient flows which minimize Lipschitz regularized f-divergences which are written in variational formulation. Variational formulation enables to approximate a function of likelihood ratio dP/dQ between two empirical distributions obtained from samples. In case of KL-divergence, this function is the log likelihood ratio. We allow flexibility in choosing f depending on the probability distribution to learn, so that heavy-tailed distributions can be fitted using alpha divergences, instead of the KL divergence. On the other hand, Lipschitz regularization leads to the f-divergences bounded even between non-absolutely continuous distributions. In terms of the transport equation of probability distributions in the Wasserstein space, the gradient flow evolves the empirical distribution in direction of the gradients of the function of likelihood ratio that are learned from data. This function is parametrized by neural networks, and its gradients give us the particle dynamics. Hence we transport the particles through the ODEs and generate more samples from the particles trajectory. Moreover, in order to reduce the dimensions, we developed our particle transportation algorithm in latent spaces and applied to high dimensional problems such as image generation and gene expression data merging.</p> <p><a href="/assets/pdf/icerm_2023_poster_hyemin.pdf">View poster</a></p>]]></content><author><name></name></author><category term="generative_modeling"/><category term="gradient_flow"/><category term="particle_transport"/><category term="optimal_transport"/><category term="poseter"/><category term="machine_learning"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a poster for Optimal Transport in Data Science - ICERM 2023.]]></summary></entry><entry><title type="html">Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow</title><link href="https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow/" rel="alternate" type="text/html" title="Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow"/><published>2022-05-31T17:39:00+00:00</published><updated>2022-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow</id><content type="html" xml:base="https://hyemingu.github.io/blog/2022/sample_generation_through_gpa_by_gradient_flow/"><![CDATA[<p><strong>Abstract</strong> This project introduces an ongoing research to generate samples from a data set where the distribution is unknown. This project keeps focus on mass transportation approach to handle the problem. First, preliminaries on mass transportation problem and gradient flows on probability measures will be briefly introduced. Then, particle descent algorithm which is equipped with a flexible measure of distance will be introduced. The experiments on the low dimensional examples elaborate the dependency of this measure of distance on the target probability distribution. Strengths of this work comes from the flexible choice of the measure of distance and an interpolated behavior between f-divergences and Γ-intergral probability metrics. Also, the efficiency of this algorithm will be seen by comparing the convergence of a different algorithm, generative adversarial network. Then, a different approach fueled by Markov chain monte carlo will be briefly discussed in application of sample generation in a high dimensional data such as image data.</p> <p><a href="/assets/pdf/hyemin_gu_2022s_m697u_project_report.pdf">View paper</a> | <a href="/assets/pdf/hyemin_gu_2022s_math697u_project_presentation.pdf">View slides</a></p>]]></content><author><name></name></author><category term="generative_modeling"/><category term="gradient_flow"/><category term="particle_transport"/><category term="optimal_transport"/><category term="class_project"/><category term="machine_learning"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2022 Spring Project paper/presentation slide for the Stochastic processes class.]]></summary></entry><entry><title type="html">Displaying External Posts on Your al-folio Blog</title><link href="https://hyemingu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/" rel="alternate" type="text/html" title="Displaying External Posts on Your al-folio Blog"/><published>2022-04-23T23:20:09+00:00</published><updated>2022-04-23T23:20:09+00:00</updated><id>https://hyemingu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog</id><content type="html" xml:base="https://hyemingu.github.io/blog/2022/displaying-external-posts-on-your-al-folio-blog/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Python ODE solving tutorial</title><link href="https://hyemingu.github.io/blog/2021/python_ode_solving/" rel="alternate" type="text/html" title="Python ODE solving tutorial"/><published>2021-12-05T17:39:00+00:00</published><updated>2021-12-05T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/python_ode_solving</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/python_ode_solving/"><![CDATA[<h2 id="1-python-tutorial-and-jupyternotebook-setup">1. Python Tutorial and JupyterNotebook Setup</h2> <p>If you need an assist to setup your computer to handle numerical computations and showing interactive results, this 15-min video would give you the first step.</p> <p><a href="/assets/pdf/video1-python_tutorial_and_jupyternotebook_setup.pdf">View slides</a> | <a href="https://youtu.be/tHtfhgSVdIY">Youtube video</a></p> <hr/> <h2 id="2python-ode-solver">2.Python ODE solver</h2> <p><a href="/assets/pdf/video2-python_ode_solver.pdf">View slides</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video2-Python_ODE_Solver.ipynb">Jupyter notebook</a> | <a href="https://youtu.be/V2EDJBX4l9w">Youtube video</a></p> <hr/> <h2 id="3-pinn-for-dynamical-systems">3. PINN for Dynamical systems</h2> <p>This 18 min video would provide you the tutorial to “Implementation and Python Library Tutorial for PINNs to Handle Dynamical Systems”.</p> <p>Lorenz model and its inverse/forward problems are chosen as the example for dynamical systems.</p> <p>I attached the modified example code for forward/inverse Lorenz model in jupyter notebook (ipnb) and the slides.</p> <p><a href="/assets/pdf/video3-pinn_for_dynamical_systems.pdf">View slides</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video3-Pytorch-Forward-HarmonicOscillator.ipynb">Jupyter notebook - pytorch PINN</a> | <a href="https://github.com/HyeminGu/Python_ODE_solving_tutorial/blob/main/video3-deepXDE-ForwardInverse-Lorenz.ipynb">Jupyter notebook - DeepXDE</a> | <a href="https://youtu.be/vR5f1gXoVbc">Youtube video</a></p>]]></content><author><name></name></author><category term="dynamical_system"/><category term="teaching"/><category term="machine_learning"/><category term="python_tutorial"/><category term="applied_mathematics"/><category term="tutorial"/><category term="Python"/><summary type="html"><![CDATA[These are 3 days tutorials(videos, slides, jupyter notebooks) for solving ODEs using Python numerical ODE solvers and introducing PINNs. It was a part of 2021 Fall Nonlinear dynamics class. Conducted by T.A. Hyemin Gu.]]></summary></entry><entry><title type="html">Lorenz Equations for Atmospheric Convection Modeling</title><link href="https://hyemingu.github.io/blog/2021/lorenz_eq/" rel="alternate" type="text/html" title="Lorenz Equations for Atmospheric Convection Modeling"/><published>2021-05-31T17:39:00+00:00</published><updated>2021-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/lorenz_eq</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/lorenz_eq/"><![CDATA[<p><strong>Abstract</strong> The Lorenz model is a dynamical system of three first order differential equations. It was designed by Lorenz as a simplified model of atmospheric convection. This project assumes that the Earth’s atmosphere is an incompressible fluid situated between two horizontal planes. Using governing equations in 2D hydrodynamics, the steps of Lorenz are followed to derive the Lorenz equations from an abstract climate model. Then, properties of the Lorenz equations are explored and illustrated by individual examples and their interpretations.</p> <p><a href="/assets/pdf/project_presentation_applied_math_modeling_hyemingu.pdf">View slides</a> | <a href="/assets/pdf/project_paper_applied_math_modeling.pdf">View paper</a></p>]]></content><author><name></name></author><category term="dynamical_system"/><category term="class_project"/><category term="applied_mathematics"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2021 Spring Project paper for the Applied Math and Math Modeling class.]]></summary></entry><entry><title type="html">Learning operators</title><link href="https://hyemingu.github.io/blog/2021/learning_operator/" rel="alternate" type="text/html" title="Learning operators"/><published>2021-05-31T17:39:00+00:00</published><updated>2021-05-31T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/learning_operator</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/learning_operator/"><![CDATA[<p><strong>Abstract</strong> As the Neural Network framework(Physics-informed Neural Network) is introduced to solve PDEs and dynamical systems, solutions of differential equations are learned from data, and this could replace or improve the conventional numerical solvers. Still, there are limitations on this “learning functions”. A new framework of “learning operators” came up in order to cure the limitations, and in specific, DeepONet provided a Neural Network architecture for learning operators which map functions to functions. This talk will show the mechanism and deeds of DeepONet(2020) and mention its follow-up studies.</p> <p><a href="/assets/pdf/learning_operators.pdf">View slides</a></p>]]></content><author><name></name></author><category term="machine_learning"/><category term="differential_equations"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a review presentation slide delivered in machine learning reading seminar in 2021.]]></summary></entry><entry><title type="html">Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space</title><link href="https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS/" rel="alternate" type="text/html" title="Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space"/><published>2021-05-30T17:39:00+00:00</published><updated>2021-05-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS</id><content type="html" xml:base="https://hyemingu.github.io/blog/2021/kernelSVM_in_RKHS/"><![CDATA[<p><strong>Abstract</strong> Support Vector Machine (SVM) provides a linear classifier for binary classification problems. Complex decision boundaries in the input feature space are handled by nonlinear kernels to the SVM. Theories in Reproducing Kernel Hilbert Spaces (RKHS) state that, given a kernel \(\mathcal{K}\) and a set of \(M\) given data \((x_i,y_i)\), for \(i=1,\cdots,M\), a SVM classifier function can be written as \(f(x) =\alpha_0 + \sum_{i=1}^M \alpha_i \mathcal{K}(x, x_i)\) for some coefficients \(\alpha_i\)s. Also, applying conformal transforms to a positive definite kernel produces another positive definite kernel which are in more complexity. Hence, in case that well-known kernels fail given the current training data, a new kernel can be tried by optimizing the coefficients of a conformal kernel in the way to maximize the ratio “(Between-class error)/(Within-class error)” of the training data. Here, data-dependent kernel SVM is applied to an application of classifying tumor/tumor-free organs from gene expression data and compared its classification performance with other well-known kernels.</p> <p><a href="/assets/pdf/project_paper_math_found_of_ai.pdf">View file</a></p>]]></content><author><name></name></author><category term="machine_learning"/><category term="class_project"/><category term="machine_learning"/><category term="presentation"/><summary type="html"><![CDATA[This is a 2021 Spring Project paper for the ST - Math Foundations of Probabilistic Artificial Intelligence II class.]]></summary></entry><entry><title type="html">R Bioinformatics 2. R Genome informatics methodology and practice.</title><link href="https://hyemingu.github.io/blog/2020/r_bioinformatics/" rel="alternate" type="text/html" title="R Bioinformatics 2. R Genome informatics methodology and practice."/><published>2020-12-30T17:39:00+00:00</published><updated>2020-12-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/r_bioinformatics</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/r_bioinformatics/"><![CDATA[<p>This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.</p> <p><a href="/assets/pdf/r_bioinformatics.pdf">View file</a></p>]]></content><author><name></name></author><category term="teaching"/><category term="r_bioinformatics"/><category term="bioinformatics"/><category term="tutorial"/><category term="R"/><summary type="html"><![CDATA[This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.]]></summary></entry><entry><title type="html">R Bioinformatics 1. R Statistics.</title><link href="https://hyemingu.github.io/blog/2020/r_statistics/" rel="alternate" type="text/html" title="R Bioinformatics 1. R Statistics."/><published>2020-09-30T17:39:00+00:00</published><updated>2020-09-30T17:39:00+00:00</updated><id>https://hyemingu.github.io/blog/2020/r_statistics</id><content type="html" xml:base="https://hyemingu.github.io/blog/2020/r_statistics/"><![CDATA[<p>This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.</p> <p><a href="/assets/pdf/r_statistics.pdf">View file</a></p>]]></content><author><name></name></author><category term="teaching"/><category term="r_bioinformatics"/><category term="bioinformatics"/><category term="tutorial"/><category term="R"/><summary type="html"><![CDATA[This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.]]></summary></entry></feed>