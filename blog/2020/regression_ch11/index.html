<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RA ch11 Remedial measures | Hyemin Gu </title> <meta name="author" content="Hyemin Gu"> <meta name="description" content="Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/kitty.png?29333e1d6c912d924a80157ab1e4b645"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyemingu.github.io/blog/2020/regression_ch11/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hyemin</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RA ch11 Remedial measures</h1> <p class="post-meta"> Created in July 10, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   <a href="/blog/tag/lecture-note"> <i class="fa-solid fa-hashtag fa-sm"></i> lecture_note</a>   ·   <a href="/blog/category/teaching"> <i class="fa-solid fa-tag fa-sm"></i> teaching</a>   <a href="/blog/category/regression-analysis"> <i class="fa-solid fa-tag fa-sm"></i> regression_analysis</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="ch-11-remedial-measures">Ch 11: Remedial measures</h1> <p>Transformation is one of the standard remedial measure for a linear model. Recall that its uses are:</p> <ol> <li>to linearize the regression relationship</li> <li>to make the error distribution more nearly normal</li> <li>to make the variances of the error terms more nearly equal.</li> </ol> <p>In this chapter, we will find additional remedial measures to handle several pitfalls. Then we discuss non-parametric regression methods (which are quite different from previous regression models). A common feature of the remedial measures and alternative regression methods is that estimation procedures from the ways we’ve seen are relatively complex, so we need easier and more generic way to evaluate the precision of these complex estimators. Bootstrapping is one example.</p> <h2 id="outline">Outline</h2> <ol> <li> <p>Other remedial measures</p> <ul> <li>unequal error variance - weighted least squares</li> <li>multicollinearity - ridge regression</li> <li>influential cases - robust regression</li> </ul> </li> <li> <p>Nonparametric regression</p> <ul> <li>regression trees</li> </ul> </li> <li> <p>Bootstrap confidence intervals</p> </li> </ol> <hr> <h2 id="1-other-remedial-measures">1. Other remedial measures</h2> <h3 id="unequal-error-variance---weighted-least-squares">unequal error variance - weighted least squares</h3> <p><strong>Generalized multiple regression model</strong> \(Y_i = \beta_0 + \beta_1 X_{i1} + \cdots + \beta_{p-1} X_{i,p-1} + \epsilon_i\) where \(e_i\)’s are independent, \(e_i \sim N(0, \sigma_i^2)\), $$i = 1,2,\cdots, n</p> <p>\(. Here, the error variances may not be equal, so that\)\sigma^2(\epsilon)\(is an\)n\times n\(diagonal matrix where each entry has\)\sigma_i^2$$.</p> <p>When we use ordinary least square estimators, then still we have these properties:</p> <ol> <li>the estimators of \(\beta\) are unbiased</li> <li>the estimators are <em>consistent</em> <em>consistency</em> is defined that \(\forall \varepsilon &gt;0, P(|b_n - \beta| \leq \varepsilon) = 1\) for succiciently large \(n\) (\(b_n\) converges to \(\beta\)). However, the estimators are no longer have minimum variance. This is due to the unequal error variances, making \(n\) cases no longer have the same reliability. In other words, observations with small variances provide more reliable information.</li> </ol> <p>A remedy to handle this problem is to use <strong>weighted least squares</strong>. First, we start from the simplest case.</p> <p><strong>when error variances are known</strong> Suppose that the errror variances \(\sigma_i^2\) for all \(n\) observations are given. Then we use <strong>Maximum likelihood method</strong>, again. First, we define the likelihood function \(L(\beta)\).</p> \[L(\beta) = \prod_{i=1}^n \frac{1}{(2\pi \sigma_i^2)^\frac{1}{2}} exp [ - \frac{1}{2 \sigma_i^2} (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2 ]\] <p>Here, we substitute \(w_i = \frac{1}{\sigma_i^2}\) and rewrite the formula.</p> \[L(\beta) = \prod_{i=1}^n \left( \frac{w_i}{2\pi}\right)^\frac{1}{2} exp [ - \frac{w_i}{2} (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2 ]\] <p>Since \(\sigma_i^2\)’s are known, parameters for likelihood function are just regression coefficients. Maximum likelihood method finds critical points which maximizes the likelihood function value.</p> <p>This is just as the same as minimizing \(Q_w = \sum_{i=1}^n w_i (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2\). The normal equation can be written using weights matrix \(W = diag(w_1, w_2, \cdots, w_n)\) as below:</p> \[X^t WX b_w = X^tW Y\] <p>\(\Rightarrow b_w = (X^t WX)^{-1} X^tW Y\) where \(\sigma^2(b_w) = (X^t WX)^{-1}\). This is called <strong>Weighted least square estimator</strong>.</p> <p><strong>when variances are known up to proportionality constant</strong> Now, relaxing the condition, assume that we only know proportions among error variances. Consider a case that the second error variance is twice larger than the first one. i.e. \(\sigma_2^2 = 2*\sigma_1^2\). Then we can write \(w_i\)’s in this form: \(w_i = \frac{1}{k_i\sigma^2}\) where \(\sigma^2\) is unknown true error variance of \(\epsilon_1\). Then, \(p \times p\) matrix \(\sigma^2 (b_w) = \sigma^2 (X^t K X)^{-1}\), where \(K = diag(k_1, k_2, \cdots, k_n)\). Since \(\sigma^2\) is unknown, we use \(s^(b_w) = MSE_w (X^t K X)^{-1}\), instead, where \(MSE_w = \frac{\sum_{i=1}^n w_i (Y_i - \hat{Y_i})^2}{n-p} = \frac{\sum_{i=1}^n w_i e_i^2}{n-p}\).</p> <p><strong>when error variances are unknown</strong> We use estimators of the error variances. There are two approaches.</p> <ol> <li>Estimation by residuals \(\sigma_i^2 = E(\epsilon^2) - [E(\epsilon)]^2 = E(\epsilon^2)\). Hence, the squared residual \(e_i^2\) is an estimator of \(\sigma_i^2\).</li> <li>Use of replicates or near replicates <ul> <li>In a designed experiment (such as scientific simulations), replicate observations could be made. If the number of replicates is large, use the fact that sample variance gets closer to variance.</li> <li>However, in an observational studies (such as social studies), we use near observations as replicates.</li> </ul> </li> </ol> <p>Inference procedures should be followed when weights are estimated. We want to estimate a variance-covariance matrix \(\sigma^2(b_w)\). The confidence intervals for regression coefficients are estimated with \(s^2(b_{w_k})\) where \(k\) stands for a bootstrap index. Then apply bootstraping method.</p> <p>Lastly, we can use the ordinary least squares leaving the error variances unequal. Regression coefficient \(b\) is still unbiased and consistent, but no longer a minimum variance estimator, where its variance is given as \(\sigma^2(b) = (X^tX)^{-1} X^t \sigma^2(\epsilon)X (X^tX)^{-1}\) and it is estimated by \(s^2(b) = (X^tX)^{-1} X^t S_0 X (X^tX)^{-1}\) where \(S_0 = diag(e_1^2, e_2^2, \cdots, e_n^2)\).</p> <p><u>Remark on weighted least squares</u> Weighted least squares can be interpreted as transforming the data \((X_i, Y_i, \epsilon_i)\) by \(W^{\frac{1}{2}}\). This can be written as</p> \[W^{\frac{1}{2}}Y = W^{\frac{1}{2}}X\beta + W^{\frac{1}{2}}\epsilon\] <p>and by setting \(Y_w = W^{\frac{1}{2}} Y\), \(X_w = W^{\frac{1}{2}} X\), and \(\epsilon_w = W^{\frac{1}{2}} \epsilon\), the transformed variables follow the ordinary linear regression model</p> \[Y_w = X_w \beta + \epsilon_w\] <p>where \(E(\epsilon_w) = W^{\frac{1}{2}} E(\epsilon) = W^{\frac{1}{2}} 0 = 0\) and \(\sigma^2(\epsilon_w) = W^{\frac{1}{2}} \sigma^2(\epsilon) W^{\frac{1}{2}} = W^{\frac{1}{2}} W^{-1} W^{\frac{1}{2}} = I\). The regression coefficients are given as \(b_w = (X_w^t X_w)^{-1} X_w^t Y_w\).</p> <h3 id="multicollinearity---ridge-regression">multicollinearity - ridge regression</h3> <p>When multicollineariry is detected in a regression model, we have fixed the model by</p> <ol> <li>using centered data in polynomial regression models</li> <li>drop some redundant predictor variables</li> <li>add some observations that could break the multicollinearity</li> <li>use pricipal components instead of the current variables \(X_k\)</li> <li> <strong>Ridge regression</strong>.</li> </ol> <p>Ridge regression perturb the estimated coefficient \(b_R\) from the unbiased estimator \(b\), and therefore it could remedy multicollinearity problem. In other words, our unbiased estimator \(b\) is obtained by the observation \(X\) with multicollinearity, but slightly perturbed estimator \(b_R\) is obtained by a virtual perturbed observation \(X_R\) without multicollinearity. When perturbing \(b_R\), we add restriction on \(b_R\) to have small norm(length). Length restriction makes the variance of the sampling distribution of \(b_R\) shrink. Although, \(b_R\) is no longer an unbiased estimator, we have more chance to obtain \(E(b_R)\) from a sampled \(b_R\) compared to obtaining \(E(b)=\beta\) from a sampled \(b\). The relationship is given as a figure.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/ridge_regression-480.webp 480w,/assets/img/regression/ridge_regression-800.webp 800w,/assets/img/regression/ridge_regression-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/regression/ridge_regression.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ridge regression </div> <p>To obtain \(b_R\), we consider using correlation transformed \(X\) observations. This process normalizes the variable \(X\) adjusting the magnitude of \(SSE = (Y-X\beta)^t (Y-X\beta)\). \(b_R\) can be obtained by minimizing a modified sum suquared error function \(Q(\beta) = (Y-X\beta)^t (Y-X\beta) + c \beta^t \beta\) where \(c\) is a given biasing coefficient and \(c \leq 0\). Squared \(\ell_2\) norm of \(\beta\) is added in the function. This allows to minimize both \(SSE\) and the length of \(\beta\), at the same time. The intensity to keep focus on minimizing the length of \(\beta\) can be adjusted by choosing the coefficient \(c\). Larger the value \(c\), the minimizing process would be more focused on minimizing the length of \(\beta\). Resulting normal equation is \((X^t X + cI) b_R = X^t Y\). Therefore, \(b_R = (X^tX + cI)^{-1} X^tY\).</p> <p><strong>Choice of biasing constant \(c\)</strong> As the biasing constant \(c\) increases, the bias of the model would be increased. But there exists a value of \(c\) for which the regression estimator \(b_R\) has a smaller total mean squared error \(E(b^R - \beta)^2\) than the ordinary LSE \(b\). But optimum value of \(c\) varies from one application to another and it is unknown. Common heuristics to determine an appropriate \(c\) are:</p> <ol> <li>using Ridge trace Try several \(c\) and plot \(b_R\)’s according to values of \(0 \leq c \leq 1\). Take the point \(c^\ast\) where the fluctuation in \(b_R\) ceases.</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/ridge_trace-480.webp 480w,/assets/img/regression/ridge_trace-800.webp 800w,/assets/img/regression/ridge_trace-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/regression/ridge_trace.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> ridge trace </div> <ol> <li>using \(VIF_k\) Try several \(c\) and plot \(VIF_k\)’s according to values of \(0 \leq c \leq 1\). Take the point \(c^\ast\) where the fluctuation in \(b_R\) ceases.</li> </ol> <h3 id="influential-cases---robust-regression">influential cases - robust regression</h3> <p>When an outlying influential case is not clearly erroneous, we should proceed a further examination to obtain important clues to improve the model. We may find</p> <ul> <li>the omission of an important predictor variable,</li> <li>incorrect functional forms, or</li> <li>needs to dampen the influence of such cases.</li> </ul> <p><strong>Robust regression</strong> is an approach of regression to make the results less rely on outlying cases. There are several ways to conduct robust regression:</p> <ol> <li>LAR (= LAD = minimum \(\ell_1\)-error) regression Least Absolute Residuals (= Least Absolute Deviations = minimum ell-one-error regression) is done by minimizing \(Q_1(\beta) = \sum_{i=1}^n | Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1} |\). When there’s an outlier where its residual is large \(e_i &gt; 1\), then its squared residual gets much larger so that the regression function could overweigh this observation. This can be alleviated by using absolute values of residuals. Note that \(\ell_1\) function is not differentiable. Thus, we cannot obtain an estimator \(\hat{b}\) using partial derivatives. Instead, this problem can be dealt by linear programming (a kind of optimization technique).</li> <li>IRLS robust regression Iteratively Reweighted Least Squares. This approach uses the weighted least square procedure as introduced before, but, weights on residuals are now determined by the magnitude of how far outlying a case is. The weights are updated iteratively in a way to flatten the weights.</li> <li>LMS regression Least Median Squares. Recall that average is sensitive to outliers, but median is not. Thus, we could minimize the function \(median (Y_i - \beta_0 - \beta_1 X_{i1} - \cdots - \beta_{p-1} X_{i,p-1})^2\).</li> </ol> <h2 id="2-nonparametric-regression">2. Nonparametric regression</h2> <p>For complex cases, it is hard to guess a right functional form (analytic expression) of the regression function. Also, as the number of parameter increases, the space of observations gets rarefied (there are fewer cases in a neighborhood), so that the regression function tends to behave erratically. For these cases, nonparametric regression could be helpful.</p> <h3 id="regression-trees">regression trees</h3> <p>Regression tree is a powerful, yet computationally simple method of nonparametric regression. It requires virtually no assumptions, and it is simple to interpret. It is popular for explanatory studies, especially for large data sets.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/regression/regression_tree-480.webp 480w,/assets/img/regression/regression_tree-800.webp 800w,/assets/img/regression/regression_tree-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/regression/regression_tree.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> regression tree </div> <p>In each branch of a regression tree, the range of a specific predictor variable \(X_k\) is partitioned into segments (one for \(X_k \leq C\) and the other for \(X_k &lt; C\)). In the terminal nodes of the tree, we evaluate the estimated regression fit by taking the mean value of the response in that segment. i.e. \(\hat{Y_i} = \sum_{j=1}^{n_s} Y_j\) where \(Y_j\)’s belong to a segment where \(i\)th observation belongs to. Also, \(n_s\) is the number of observations in the segment.</p> <p>How to find a “best” regression tree is linked with</p> <ul> <li>the number of segmented regions \(r\), and</li> <li>the split points \(X_k\) that optimally divides the data into two sets at each branch.</li> </ul> <p>The basic idea is to choose a model which minimizes the error sum of squres for the resulting regression tree. \(SSE = \sum_{t_i \in T} SSE(t_i)\), where \(T\) is the set of all terminal nodes, and \(SSE(t_i) = \sum_{j=1}^n (Y_j - \overline{Y_{t_i}})^2\). Here, \(MSE = SSE/(n-r)\)) and \(R^2 = 1- \frac{SSE}{SSTO}\). The number of candidate models are \(r(p-1)\) where \(1 \leq r \leq n\). The value of \(r\) is usually chosen through validation studies as \(argmin_r{MSPR}\).</p> <h2 id="3-bootstrap-confidence-intervals">3. Bootstrap confidence intervals</h2> <p>Above regression approaches are complex so that it is hard to estimate confidence intervals using the previous analysis on \(\sigma^2(b)\) or \(s^2(b)\). Instead, we can approximate confidence intervals by <strong>bootstrap method</strong>. There are many different procedures to gain bootstrap confidence intervals. One example is <em>reflection method</em>.</p> <p>An \(1-\alpha\) confidence interval of a parameter \(\beta\) is \(b - (b^\ast(1-\alpha/2) - b) \leq \beta \leq b + (b - b^\ast(\alpha/2))\) where \(b^\ast(1-\alpha/2)\) and \(b^\ast(\alpha/2)\) are obtained from bootstrap distribution. $$</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/SIAM_MDS_talk_w1w2flow/">Wasserstein Proximals Stabilize Training of Generative Models and Learn Manifolds</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/SIAM_MDS_poster_GPA/">Lipschitz-Regularized Gradient Flows and Latent Generative Particles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ICERM_poster_GPA/">Lipschitz Regularized Gradient Flows and Latent Generative Particles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/">Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/python_ode_solving/">Python ODE solving tutorial</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hyemin Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"Github repositories for coding works.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-wasserstein-proximals-stabilize-training-of-generative-models-and-learn-manifolds",title:"Wasserstein Proximals Stabilize Training of Generative Models and Learn Manifolds",description:"This is a talk slide for SIAM Mathematics of Data Science 2024.",section:"Posts",handler:()=>{window.location.href="/blog/2024/SIAM_MDS_talk_w1w2flow/"}},{id:"post-lipschitz-regularized-gradient-flows-and-latent-generative-particles",title:"Lipschitz-Regularized Gradient Flows and Latent Generative Particles",description:"This is a poster for SIAM Mathematics of Data Science 2024.",section:"Posts",handler:()=>{window.location.href="/blog/2024/SIAM_MDS_poster_GPA/"}},{id:"post-lipschitz-regularized-gradient-flows-and-latent-generative-particles",title:"Lipschitz Regularized Gradient Flows and Latent Generative Particles",description:"This is a poster for Optimal Transport in Data Science - ICERM 2023.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ICERM_poster_GPA/"}},{id:"post-sample-generation-from-unknown-distributions-particle-descent-algorithm-induced-by-f-\u03b3-gradient-flow",title:"Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, \u0393)-gradient...",description:"This is a 2022 Spring Project paper/presentation slide for the Stochastic processes class.",section:"Posts",handler:()=>{window.location.href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/"}},{id:"post-python-ode-solving-tutorial",title:"Python ODE solving tutorial",description:"These are 3 days tutorials(videos, slides, jupyter notebooks) for solving ODEs using Python numerical ODE solvers and introducing PINNs. It was a part of 2021 Fall Nonlinear dynamics class. Conducted by T.A. Hyemin Gu.",section:"Posts",handler:()=>{window.location.href="/blog/2021/python_ode_solving/"}},{id:"post-learning-operators",title:"Learning operators",description:"This is a review presentation slide delivered in machine learning reading seminar in 2021.",section:"Posts",handler:()=>{window.location.href="/blog/2021/learning_operator/"}},{id:"post-lorenz-equations-for-atmospheric-convection-modeling",title:"Lorenz Equations for Atmospheric Convection Modeling",description:"This is a 2021 Spring Project paper for the Applied Math and Math Modeling class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/lorenz_eq/"}},{id:"post-data-dependent-kernel-support-vector-machine-classifiers-in-reproducing-kernel-hilbert-space",title:"Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space",description:"This is a 2021 Spring Project paper for the ST - Math Foundations of Probabilistic Artificial Intelligence II class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/kernelSVM_in_RKHS/"}},{id:"post-r-bioinformatics-2-r-genome-informatics-methodology-and-practice",title:"R Bioinformatics 2. R Genome informatics methodology and practice.",description:"This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_bioinformatics/"}},{id:"post-r-bioinformatics-1-r-statistics",title:"R Bioinformatics 1. R Statistics.",description:"This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_statistics/"}},{id:"post-ra-ch12-autocorrelation-in-time-series-data",title:"RA ch12 Autocorrelation in time series data",description:"Lecture note chapter 12 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch12/"}},{id:"post-ra-ch11-remedial-measures",title:"RA ch11 Remedial measures",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch11/"}},{id:"post-ra-ch10-diagnostics",title:"RA ch10 Diagnostics",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch10/"}},{id:"post-ra-ch9-model-selection-and-validation",title:"RA ch9 Model selection and validation",description:"Lecture note chapter 9 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch9/"}},{id:"post-ra-ch8-regression-models-for-quantitative-and-qualitative-predictors",title:"RA ch8 Regression models for Quantitative and Qualitative predictors",description:"Lecture note chapter 8 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch8/"}},{id:"post-ra-ch7-multiple-regression-2",title:"RA ch7 Multiple Regression 2",description:"Lecture note chapter 7 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch7/"}},{id:"post-ra-ch6-multiple-regression-1",title:"RA ch6 Multiple Regression 1",description:"Lecture note chapter 6 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch6/"}},{id:"post-ra-ch5-matrix-approaches-to-simple-linear-regression",title:"RA ch5 Matrix Approaches to Simple Linear Regression",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch5/"}},{id:"post-ra-ch4-diagnostics-and-remedial-measures",title:"RA ch4 Diagnostics and Remedial Measures",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch4/"}},{id:"post-ra-ch3-inferences-in-regression-and-correlation-analysis",title:"RA ch3 Inferences in Regression and Correlation Analysis",description:"Lecture note chapter 3 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch3/"}},{id:"post-ra-ch2-review-of-mathematical-statistics",title:"RA ch2 Review of Mathematical Statistics",description:"Lecture note chapter 2 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch2/"}},{id:"post-ra-ch1-linear-regression-with-one-predictor-variable",title:"RA ch1 Linear Regression with One Predictor Variable",description:"Lecture note chapter 1 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch1/"}},{id:"post-training-a-2-layer-neural-network-using-svd-generated-weights",title:"Training a 2 layer Neural Network using SVD-generated weights",description:"This is a poster for Joint Mathematics Meetings 2018.",section:"Posts",handler:()=>{window.location.href="/blog/2018/training_nn_from_SVD_weights/"}},{id:"post-necessary-and-sufficient-conditions-for-shortest-vectors-in-lattices-of-low-dimension",title:"Necessary and sufficient conditions for shortest vectors in lattices of low dimension.",description:"This is a poster for Joint Mathematics Meetings 2017.",section:"Posts",handler:()=>{window.location.href="/blog/2017/necessary_and_sufficient_conditions_for_shortest_vectors_in_lattices_of_low_dimension/"}},{id:"news-initiated-a-role-as-a-research-assistant-advised-by-markos-katsoulakis",title:"Initiated a role as a research assistant, advised by Markos Katsoulakis.",description:"",section:"News"},{id:"news-initiated-a-role-as-a-twigs-coordinator",title:"Initiated a role as a TWIGS coordinator.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-passed-oral-exam",title:"Passed oral exam.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_3/"}},{id:"news-presented-a-poster-at-optimal-transport-in-data-science-icerm-brown-university",title:"Presented a poster at Optimal Transport in Data Science \u2013 ICERM, Brown university....",description:"",section:"News"},{id:"news-paper-for-lipschitz-regularized-generative-particles-algorithm-was-published-at-siam-data-science",title:"Paper for Lipschitz regularized generative particles algorithm was published at SIAM Data Science....",description:"",section:"News"},{id:"news-paper-for-wasserstein-1-wasserstein-2-generative-flow-was-released-at-arxiv",title:"Paper for Wasserstein-1/Wasserstein-2 generative flow was released at Arxiv.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%67%75@%75%6D%61%73%73.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Hyemin-Gu-2/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HyeminGu","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hyemin-gu-58127a207","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/hyemin.gu.9022","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>