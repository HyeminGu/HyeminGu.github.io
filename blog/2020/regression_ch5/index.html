<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RA ch5 Matrix Approaches to Simple Linear Regression | Hyemin Gu </title> <meta name="author" content="Hyemin Gu"> <meta name="description" content="Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/kitty.png?29333e1d6c912d924a80157ab1e4b645"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyemingu.github.io/blog/2020/regression_ch5/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hyemin</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RA ch5 Matrix Approaches to Simple Linear Regression</h1> <p class="post-meta"> Created in July 04, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   <a href="/blog/tag/lecture-note"> <i class="fa-solid fa-hashtag fa-sm"></i> lecture_note</a>   ·   <a href="/blog/category/teaching"> <i class="fa-solid fa-tag fa-sm"></i> teaching</a>   <a href="/blog/category/regression-analysis"> <i class="fa-solid fa-tag fa-sm"></i> regression_analysis</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="ch-5-matrix-approaches-to-simple-linear-regression">Ch 5: Matrix Approaches to Simple Linear Regression</h1> <p>Linear functions can be written by matrix operations such as addition and multiplication. Now, we move on to formulation of linear regression into matrices. Knowledge of linear algebra provides lots of intuition to interpret linear regression models. Also, it is easier to describe the calculations using matrices, even in the generalized settings. Therefore, we briefly review useful concepts in linear algebra, and then describe the simple linear regression model into matrix form.</p> <h2 id="outline">Outline</h2> <ol> <li>Review of linear algebra <ul> <li>Linear indep:endency and rank</li> <li>quadratic form</li> <li>projection(idempotent) matrix</li> </ul> </li> <li>Simple linear regression into matrix form <ul> <li>notations</li> <li>simple linear regression</li> <li>ANOVA results</li> <li>a sketch to generalized settings</li> </ul> </li> </ol> <hr> <h2 id="1-review-of-linear-algebra">1. Review of linear algebra</h2> <h3 id="linear-independency-and-rank">Linear independency and rank</h3> <p><strong>Linear independency</strong> among a set of vectors \(\{ v_1, v_2, \cdots, v_n \}\) is defined as any linear combination leading to the zero vector \(c_1v_1 + c_2v_2 + \cdots + c_nv_n=0\) implies \(c_1=c_2=\cdots=c_n=0\). We can also consider the linear independency among row/column vectors in a square matrix \(M \in \mathbb{R}^{n,n}\) where \(M = [v_1 v_2 \cdots v_n]\). About linear independency, the following are equivalent:</p> <ol> <li>\(\{v_1, v_2, \cdots, v_n\}\) is linearly independent.</li> <li>\(Mx=b\) is consistent for any \(b\). (nonsingular)</li> <li>\(Mx=b\) has the unique solution.</li> <li>\(Mx=0\) has only a trivial solution (zero vector). \(\iff\) Nullspace of \(M\) is trivial.</li> <li> \[det(M)\neq0\] </li> <li>Eigenvalues of \(M\) are nonzero.</li> <li>\(M\) is invertible.</li> </ol> <p>For a square matrix \(M\), <strong>rank</strong> is the number of linearly independent rows/columns of \(M\). Rank cannot exceed the dimension of the vector space (Since \(M\) is a mapping (function) which maps a vector in \(\mathbb{R}^n\) to \(\mathbb{R}^n\), the dimension of the vector space is \(n\).) If rank equals \(n\), we say the matrix \(M\) is full-rank, and otherwise, rank-deficient.</p> <p>What about a general matrix \(M\in \mathbb{R}^{m,n}\)? In this case, rank is defined as the same way as above. But in the case of overdetermined system \(m &gt; n\), rank is up to \(min(m,n)=n\), so we may not have an exact solution. Also, \(M^tM \in \mathbb{R}^{n,n}\) could be full-rank only if all the columns were independent(rank equals to \(n\).), whereas \(MM^t \in \mathbb{R}^{m,m}\) never becomes full-rank.</p> <h3 id="quadratic-form">quadratic form</h3> <p>Let \(x = (x_1 x_2 \cdots x_n)\) be a vector with \(n\) entries/variables, and \(A\) be a \(n\) by \(n\) square matrix. <strong>\(x^tAx\)</strong> is called a quadratic form of \(x\) with respect to \(A\) because it represents <em>any quadratic function</em> of variable \(x\). \(\)x^tAx = a<em>{11}x_1^2 + (a</em>{12} + a<em>{21})x_1x_2 + (a</em>{13} + a<em>{31})x_1x_3 + \cdots + a</em>{nn}x_n^2\(\)</p> <p>Moreover, if \(A\) is positive definite (\(x^tAx &gt; 0\) except for \(x=0\) \(\iff\) Eigenvalues of \(A\) are all positive), the quadratic form is a convex function(볼록함수), which has a global minimum.</p> <h3 id="projectionidempotent-matrix">projection(idempotent) matrix</h3> <p>A matrix \(H\) is said to be <strong>idempotent</strong> if it satisfies \(H^2=H\). Another name of such \(H\) is projection matrix. Several peroperties of idempotent matrices are:</p> <ol> <li>Any eigenvalue of an idempotent matrix is 1 or 0.</li> <li>Trace(sum of diagonal entries) of an idempotent matrix becomes its rank.</li> <li>If \(H\) is idempotent, then \(I-H\) is also idempotent. <br>(Moreover, it decomposes the vector space \(\mathbb{R}^n\) into 2 spaces as \(\mathbb{R}^n = range(H) \oplus range(I-H)\) where \(\oplus\) denotes direct sum. \(\iff\) <br>\(range(H)=null(I-H), range(I-H)=null(H)\). This concept is different from orthogonality. Orthogonality is a stronger condition.)</li> <li>Any projection matrix except for \(I\) is rank-deficient (not invertible).</li> </ol> <p>When an idempotent matrix \(H\) also satisfies \(H=H^t\) (symmetric) then it is called an <strong>orthogonal projection matrix</strong>. For any matrix \(X\), \(X(X^tX)^{-1}X^t\) is an orthogonal projection, and it projects a vector into range space (column space) of \(X\).</p> <p><u>Remark</u> A matrix \(U\in \mathbb{R}^{n,n}\) satisfying \(U^tU=UU^t=I\) is called an orthogonal matrix. This means \(U\) is invertible with its inverse \(U^t\). Be careful that orthogonal projection matrix is not an orthogonal matrix!</p> <h2 id="2-simple-linear-regression-into-matrix-form">2. Simple linear regression into matrix form</h2> <h3 id="notations">notations</h3> <p>Response variable \(Y = (Y_1, Y_2, \cdots, Y_n)^t\) Design matrix \(X = \newline(1, X_{11}; 1, X_{21}; \vdots ~~ \vdots; 1, X_{n1})\) Coefficient vector \(\beta = (\beta_1, \beta_2, \cdots, \beta_n)^t\) Error vector \(\epsilon = (\epsilon_1, \epsilon_2, \cdots, \epsilon_n)^t\)</p> \[E(Y) = (E(Y_1), E(Y_2), \cdots, E(Y_n))^t = X\beta\] \[\sigma^2(Y) = E[ (Y-E(Y))(Y-E(Y))^t ]\] \[E(\epsilon) = 0\] \[\sigma^2(\epsilon) = \sigma^2 I\] <p>\(1 = (1, \cdots, 1)^t\) (one-vector) \(J\) : A matrix whose entries are all 1 (one-matrix)</p> <p><u>Note</u></p> <ul> <li> \[Y^tY = \sum_i Y_i^2\] </li> <li> \[X^tY = (\sum_i Y_i \sum_i X_iY_i)^t\] </li> <li> \[X^tX = (n, \sum_i X_i; \sum_i X_i, \sum_i X_i^2)\] </li> <li>\((X^tX)^{-1} = (\frac{\sum_i X_i^2}{n\sum_i(X_i-\bar{X})^2} , \frac{-\sum_i X_i}{n\sum_i(X_i-\bar{X})^2}; \frac{-\sum_i X_i}{n\sum_i(X_i-\bar{X})^2}, \frac{n}{n\sum_i(X_i-\bar{X})^2})\) <br> \(= \frac{1}{\sigma^2} (\sigma^2(b_0), \sigma(b_0,b_1); \sigma(b_0,b_1) ,\sigma^2(b_1))\)</li> </ul> <p><u>Note</u> \(A\) : a constant matrix, \(Y\) : a random variable, and \(W = AY\) Then, \(E(A) = A\), \(E(W) = E(AY) = AE(Y)\), \(\sigma^2(W) = \sigma^2(AY) = A \sigma^2(Y) A^t\)</p> <h3 id="simple-linear-regression">simple linear regression</h3> <p>\(Y = X\beta + \epsilon\), \(E(\epsilon) = 0\), \(\sigma^2(\epsilon) = \sigma^2 I\)</p> <p><strong>Least square method, regression coefficients \(b\)</strong> Let \(b\) be a solution to the system below.</p> <ol> <li> \[nb_0 + b_1 \sum_i X_i = \sum_i Y_i\] </li> <li> \[b_0 \sum_i X_i + b_1 \sum_i X_i^2 = \sum_i X_i Y_i\] </li> </ol> <p>This can be written as \((n, \sum_i X_i; \sum_i X_i, \sum_i X_i^2)(b_0, b_1)^t = (\sum_i Y_i \sum_i X_iY_i)^t\) i.e. <strong>\(X^tXb = X^tY\)</strong> \(\Rightarrow\) <strong>\(b = (X^tX)^{-1}X^tY\)</strong> where \(X^tX\) is assumed to be full-rank (rank=2 in this case)</p> <p>properties of \(b\)</p> <ul> <li>\(E(b) = \beta\) <blockquote> \[E(b) = E[(X^tX)^{-1}X^tY] = (X^tX)^{-1}X^tE(Y) = (X^tX)^{-1}X^tX\beta = \beta\] </blockquote> </li> <li> \[\sigma^2(b) = \sigma^2(X^tX)^{-1}\] </li> </ul> <p><strong>Objective function, \(Q\)</strong> \(Q = \sum_i \left(Y_i - (\beta_0 + \beta_1 X_i)\right)^2 = (Y-X\beta)^t(Y-X\beta) = Y^tY - 2\beta^tX^tY +\beta^tX^tX\beta\) \(Q\) is a quadratic function of variable \(\beta\). Note that the term \(Y^tY = Y^tIY\) is a quadratic form and since \(I\) is positive definite, \(Q\) has a global minimum. We can find a solution by finding a critical point. \(b\) is a solution of \(\frac{\partial Q}{\partial \beta} = -2 X^tY + 2X^tX\beta = 0\)</p> <p><strong>Regression line, \(\hat{Y}\)</strong> <strong>\(\hat{Y}\)</strong> \(= (\hat{Y_1}, \hat{Y_2}, \cdots, \hat{Y_n})^t = Xb =\) <strong>\(X(X^tX)^{-1}X^tY = HY\)</strong>, where <strong>\(H = X(X^tX)^{-1}X^t\) (Hat matrix)</strong>.</p> <p>Properties of hat matrix \(H\)</p> <ul> <li>\(H^2 = H\) (idempotent), \(H^t = H\) \(\Rightarrow\) Orthogonal projection matrix We can interpret the regression line \(\hat{Y}\) as an orthogonal projection of \(Y\) onto the range space of \(X\), \(Range(X)\).</li> <li> \[Tr(H) = rank(H) = 2\] </li> </ul> <p><strong>Residuals, \(e\)</strong> \(e = Y-\hat{Y} = Y - Xb = Y - HY = (I-H)Y\)</p> <p>Property of the residual \(e\)</p> <ul> <li>Since \(H\) is an orthogonal projection matrix, so is \(I-H\). We can interpret the residual \(e\) as an orthogonal projection of \(Y\) onto the space orthogonal to \(Range(X)\). \(e \perp X\), \(e \perp \hat{Y}\).</li> <li>\(E(e) = 0\) <blockquote> <p>\(\because E(e) = E[(I-H)Y] = (I-H) E(Y) = (I-H)X\beta\) &gt; \(= X\beta - X(X^tX)^{-1}X^tX\beta = 0\)</p> </blockquote> </li> <li>\(\sigma^2(e) = \sigma^2(I-H)\) <blockquote> <p>\(\because \sigma^2(e) = \sigma^2[(I-H)Y] = (I-H)\sigma^2(Y)(I-H)^t\) &gt; \(= (I-H)\sigma^2I(I-H) = \sigma^2(I-H)^2I = \sigma^2(I-H)\)</p> </blockquote> </li> </ul> <p><strong>Mean response of \(\hat{Y_h}\)</strong> \(\hat{Y_h} = X_h^tb\) \(\sigma^2(\hat{Y_h}) = \sigma^2(X_hb) = \sigma^2 [\frac{1}{n} + \frac{X_h-\bar{X})^2}{\sum_i(X_i-\bar{X})^2} ]\)</p> <blockquote> \[\sigma^2(X_hb) = X_h^t \sigma^2(b) X_h = X_h^t \sigma^2(X^tX)^{-1} X_h = \sigma^2 [\frac{1}{n} + \frac{X_h-\bar{X})^2}{\sum_i(X_i-\bar{X})^2}]\] </blockquote> <h3 id="anova-results">ANOVA results</h3> <ul> <li> \[SSTO = \sum_i (Y_i - \bar{Y})^2 = \sum_i Y_i^2 - \frac{(\sum_i Y_i)^2}{n} = Y^tY - \frac{1}{n}Y^tJY\] </li> <li> \[SSE = \sum_i e_i^2 = [(I-H)Y]^t[(I-H)Y] = Y^t(I-H)Y = Y^tY - Y^tHY\] </li> <li> \[SSR = SSTO-SSE = Y^tHY - \frac{1}{n}Y^tJY\] </li> </ul> <h3 id="a-sketch-to-generalized-settings">a sketch to generalized settings</h3> <ul> <li>In application, we use normal error regression model by assuming normal distribution to errors. Also, there is an explicit formula to write normal distribution of the error in matrix form.</li> <li>By moving on to matrix formulation, we can generalize the current regression model with one prediction variable to multiple variables. In most cases, the difference is just adding more variables to the design matrix \(X\) and coeffient \(\beta\) and \(b\). When we use normal error regression model for multiple regression, we have MANOVA analogous to ANOVA.</li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ICERM_poster_GPA/">Lipschitz Regularized Gradient Flows and Latent Generative Particles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/">Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/python_ode_solving/">Python ODE solving tutorial</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/learning_operator/">Learning operators</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/lorenz_eq/">Lorenz Equations for Atmospheric Convection Modeling</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hyemin Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"Github repositories for coding works.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-lipschitz-regularized-gradient-flows-and-latent-generative-particles",title:"Lipschitz Regularized Gradient Flows and Latent Generative Particles",description:"This is a poster for Optimal Transport in Data Science - ICERM 2023.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ICERM_poster_GPA/"}},{id:"post-sample-generation-from-unknown-distributions-particle-descent-algorithm-induced-by-f-\u03b3-gradient-flow",title:"Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, \u0393)-gradient...",description:"This is a 2022 Spring Project paper/presentation slide for the Stochastic processes class.",section:"Posts",handler:()=>{window.location.href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/"}},{id:"post-python-ode-solving-tutorial",title:"Python ODE solving tutorial",description:"These are 3 days tutorials(videos, slides, jupyter notebooks) for solving ODEs using Python numerical ODE solvers and introducing PINNs. It was a part of 2021 Fall Nonlinear dynamics class. Conducted by T.A. Hyemin Gu.",section:"Posts",handler:()=>{window.location.href="/blog/2021/python_ode_solving/"}},{id:"post-learning-operators",title:"Learning operators",description:"This is a review presentation slide delivered in machine learning reading seminar in 2021.",section:"Posts",handler:()=>{window.location.href="/blog/2021/learning_operator/"}},{id:"post-lorenz-equations-for-atmospheric-convection-modeling",title:"Lorenz Equations for Atmospheric Convection Modeling",description:"This is a 2021 Spring Project paper for the Applied Math and Math Modeling class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/lorenz_eq/"}},{id:"post-data-dependent-kernel-support-vector-machine-classifiers-in-reproducing-kernel-hilbert-space",title:"Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space",description:"This is a 2021 Spring Project paper for the ST - Math Foundations of Probabilistic Artificial Intelligence II class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/kernelSVM_in_RKHS/"}},{id:"post-r-bioinformatics-2-r-genome-informatics-methodology-and-practice",title:"R Bioinformatics 2. R Genome informatics methodology and practice.",description:"This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_bioinformatics/"}},{id:"post-r-bioinformatics-1-r-statistics",title:"R Bioinformatics 1. R Statistics.",description:"This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_statistics/"}},{id:"post-ra-ch12-autocorrelation-in-time-series-data",title:"RA ch12 Autocorrelation in time series data",description:"Lecture note chapter 12 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch12/"}},{id:"post-ra-ch11-remedial-measures",title:"RA ch11 Remedial measures",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch11/"}},{id:"post-ra-ch10-diagnostics",title:"RA ch10 Diagnostics",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch10/"}},{id:"post-ra-ch9-model-selection-and-validation",title:"RA ch9 Model selection and validation",description:"Lecture note chapter 9 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch9/"}},{id:"post-ra-ch8-regression-models-for-quantitative-and-qualitative-predictors",title:"RA ch8 Regression models for Quantitative and Qualitative predictors",description:"Lecture note chapter 8 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch8/"}},{id:"post-ra-ch7-multiple-regression-2",title:"RA ch7 Multiple Regression 2",description:"Lecture note chapter 7 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch7/"}},{id:"post-ra-ch6-multiple-regression-1",title:"RA ch6 Multiple Regression 1",description:"Lecture note chapter 6 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch6/"}},{id:"post-ra-ch5-matrix-approaches-to-simple-linear-regression",title:"RA ch5 Matrix Approaches to Simple Linear Regression",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch5/"}},{id:"post-ra-ch4-diagnostics-and-remedial-measures",title:"RA ch4 Diagnostics and Remedial Measures",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch4/"}},{id:"post-ra-ch3-inferences-in-regression-and-correlation-analysis",title:"RA ch3 Inferences in Regression and Correlation Analysis",description:"Lecture note chapter 3 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch3/"}},{id:"post-ra-ch2-review-of-mathematical-statistics",title:"RA ch2 Review of Mathematical Statistics",description:"Lecture note chapter 2 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch2/"}},{id:"post-ra-ch1-linear-regression-with-one-predictor-variable",title:"RA ch1 Linear Regression with One Predictor Variable",description:"Lecture note chapter 1 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch1/"}},{id:"post-training-a-2-layer-neural-network-using-svd-generated-weights",title:"Training a 2 layer Neural Network using SVD-generated weights",description:"This is a poster for Joint Mathematics Meetings 2018.",section:"Posts",handler:()=>{window.location.href="/blog/2018/training_nn_from_SVD_weights/"}},{id:"post-necessary-and-sufficient-conditions-for-shortest-vectors-in-lattices-of-low-dimension",title:"Necessary and sufficient conditions for shortest vectors in lattices of low dimension.",description:"This is a poster for Joint Mathematics Meetings 2017.",section:"Posts",handler:()=>{window.location.href="/blog/2017/necessary_and_sufficient_conditions_for_shortest_vectors_in_lattices_of_low_dimension/"}},{id:"news-initiated-a-role-as-a-research-assistant-advised-by-markos-katsoulakis",title:"Initiated a role as a research assistant, advised by Markos Katsoulakis.",description:"",section:"News"},{id:"news-initiated-a-role-as-a-twigs-coordinator",title:"Initiated a role as a TWIGS coordinator.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-passed-oral-exam",title:"Passed oral exam.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_3/"}},{id:"news-presented-a-poster-at-optimal-transport-in-data-science-icerm-brown-university",title:"Presented a poster at Optimal Transport in Data Science \u2013 ICERM, Brown university....",description:"",section:"News"},{id:"news-paper-for-lipschitz-regularized-generative-particles-algorithm-was-published-at-siam-data-science",title:"Paper for Lipschitz regularized generative particles algorithm was published at SIAM Data Science....",description:"",section:"News"},{id:"news-paper-for-wasserstein-1-wasserstein-2-generative-flow-was-released-at-arxiv",title:"Paper for Wasserstein-1/Wasserstein-2 generative flow was released at Arxiv.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%67%75@%75%6D%61%73%73.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Hyemin-Gu-2/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HyeminGu","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hyemin-gu-58127a207","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/hyemin.gu.9022","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>