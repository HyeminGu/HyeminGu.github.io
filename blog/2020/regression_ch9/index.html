<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> RA ch9 Model selection and validation | Hyemin Gu </title> <meta name="author" content="Hyemin Gu"> <meta name="description" content="Lecture note chapter 9 for regression analysis taught by Hyemin Gu in 2020"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/kitty.png?29333e1d6c912d924a80157ab1e4b645"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://hyemingu.github.io/blog/2020/regression_ch9/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Hyemin</span> Gu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">RA ch9 Model selection and validation</h1> <p class="post-meta"> Created in July 08, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/tag/statistics"> <i class="fa-solid fa-hashtag fa-sm"></i> statistics</a>   <a href="/blog/tag/lecture-note"> <i class="fa-solid fa-hashtag fa-sm"></i> lecture_note</a>   <a href="/blog/tag/tutorial"> <i class="fa-solid fa-hashtag fa-sm"></i> tutorial</a>   ·   <a href="/blog/category/teaching"> <i class="fa-solid fa-tag fa-sm"></i> teaching</a>   <a href="/blog/category/regression-analysis"> <i class="fa-solid fa-tag fa-sm"></i> regression_analysis</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="ch-9-model-selection-and-validation">Ch 9: Model selection and validation</h1> <p>When we have \(p-1\) predictor varaibles, we can construct \(2^{p-1}\) different linear models by choosing the parameters to use. Complex models could reduce sum squared error \(SSE\) significantly, but it may cause overfitting, lacking adaptability to unseen cases. Therefore, when we choose a model, we need to consider 2 conditions below at the same time.</p> <ul> <li>causing small error which can be estimated by \(SSE\)</li> <li>simple enough to perform well with new data</li> </ul> <p>This chapter focuses on introducing several criteria to choose a good linear regression model. Some could be only used(or has proved) in linear regression, while others are used in general models.</p> <h2 id="outline">Outline</h2> <ol> <li>Criteria for model selection <ul> <li>coefficient of multiple determination, \(R_p^2\)</li> <li>adjusted coefficient of multiple determination, \(R_{a,p}^2\)</li> <li>Mallow’s \(C_p\)</li> <li>\(AIC_p\) and \(BIC_p\)</li> <li>Prediction sum of squares, \(PRESS_p\)</li> </ul> </li> <li>Model validation</li> </ol> <hr> <h2 id="1-criteria-for-model-selection">1. Criteria for model selection</h2> <p>Criteria for model selection below a model with</p> <ul> <li>small \(SSE\)</li> <li>small number of parameters \(p\).</li> </ul> <h3 id="coefficient-of-multiple-determination-r_p2">coefficient of multiple determination, \(R_p^2\)</h3> \[R_p^2 = 1 - \frac{SSE_p}{SSTO}\] <p>Larger is better. It can be used in general models. drawback: always find the most complex model</p> <h3 id="adjusted-coefficient-of-multiple-determination-r_ap2">adjusted coefficient of multiple determination, \(R_{a,p}^2\)</h3> \[R_{a,p}^2= 1 - \frac{SSE_p/(n-p)}{SSTO/(n-1)}\] <p>adjusted version of \(R^2\) Larger is better. It can be used in general models.</p> <h3 id="mallows-c_p">Mallow’s \(C_p\)</h3> <p>Consider the sum of square of model error to the mean response \(\sum_i (\hat{Y_i} - E(Y_i))^2\). It could be divided into two components.</p> <ul> <li>\(\sum_i (\hat{Y_i} - E(\hat{Y_i}))^2 = \sum_i \sigma_i^2(\hat{Y_i})\) : <strong>random error</strong> caused by random noise (cannot be handled by the model itself.)</li> <li>\(\sum_i (E(\hat{Y_i}) - E(Y_i))^2\) : <strong>bias</strong> of the model</li> </ul> <p>We have a criterion measure <strong>\(\Gamma_p = \frac{1}{\sigma^2} \sum_{i=1}^n (\hat{Y_i} - E(Y_i))^2 = \sum_i \sigma_i^2(\hat{Y_i}) + \sum_i (E(\hat{Y_i}) - E(Y_i))^2\)</strong>. But there are some problems:</p> <ol> <li>We are not aware of \(\sigma^2\) in general.</li> <li>We cannot obtain \(E(Y_i)\).</li> </ol> <p>Let us have the full model with \(P-1\) predictor variables, and each model that we want to assess has \(p-1\) predictor variables.</p> <ul> <li>To settle for 1, we use \(MSE_{full} = MSE(X_1, X_2, \cdots, X_{P-1})\) (\(MSE\) of the full model) rather than \(\sigma^2\).</li> <li>To resolve 2, we use the formula \(E(SSE_p) = \sum_i (E(\hat{Y_i}) - E(Y_i))^2 + (n-p)\sigma^2\). (You may prove this using the fact that \(Tr(Var(\hat{Y}) = Tr(\sigma^2H) = p\sigma^2\).) <br>Therefore, \(\Gamma_p = \frac{1}{\sigma^2} [E(SSE_p) - (n-2p)\sigma^2 ]\). <blockquote> <p>\(\because \Gamma_p \sigma^2 = \sum_i (E(\hat{Y_i}) - E(Y_i))^2 + (n-p)\sigma^2 - (n-p)\sigma^2 + \sum_i \sigma_i^2(\hat{Y_i})\) Since \(\sigma_i^2(\hat{Y_i}) = p\sigma^2\), \(\Gamma_p = E(SSE_p) - (n-2p)\sigma^2\).</p> </blockquote> </li> </ul> <p>Now we substitute \(\Gamma_p = \frac{1}{\sigma^2} [E(SSE_p) - (n-2p)\sigma^2 ]\) with <strong>\(C_p = \frac{SSE_p}{MSE_{full}} - (n-2p)\)</strong>.</p> <p>Note that when there is no bias in the model with \(p-1\) predictor variables, then \(\Gamma_p = p\).</p> <blockquote> <p>\(\because \frac{1}{\sigma^2}\sum_i \sigma^2(\hat{Y_i})^2 = \frac{1}{\sigma^2} p \sigma^2 = p\).</p> </blockquote> <p>By the way, \(C_p\) (an estimator of \(\Gamma_p\)) becomes \(p\) when it is the full model.</p> <blockquote> <p>\(\because \frac{SSE_p}{MSE_p} - (n-2p) = n-p - (n-2p) = p\).</p> </blockquote> <p>Therefore, Smaller \(C_p\) near \(p\) is better. \(C_p\) formula is unique for assessing regression model. For other models, you should analyze \(\Gamma_p\) for their situation.</p> <h3 id="aic_p-and-bic_p">\(AIC_p\) and \(BIC_p\)</h3> <p>\(AIC_p = n log(SSE_p) - n log(n) + 2p\) \(BIC_p = n log(SSE_p) - n log(n) + [log(n)]p\) where \([k]\) stands for the largest integer which does not exceed \(k\)</p> <p>Smaller is better. It is a simple criterion.</p> <p><u>Note</u> \(BIC_p\) is more sensitive to \(p\) (penalize more to large \(p\)). To be exact, if \(n\geq 8\), then \(AIC_p \leq BIC_p\).</p> <h3 id="prediction-sum-of-squares-press_p">Prediction sum of squares, \(PRESS_p\)</h3> <p>\(PRESS_p\) doesn’t penalize a model with many predictor variables. Instead, it directly calculate prediction error to unseen data. \(PRESS_p = \sum_i (Y_i - \hat{Y_{i(i)}})^2\) where \(\hat{Y_{i(i)}}\) denotes \(i\)th fitted value using a fitted line constructed without \((X_i, Y_i)\) data.</p> <p>Smaller is better. It can be used in general models. However if the number of predictior variables are large, the computation is heavy. Therefore, we don’t go through all the candidate models, but use <em>stepwise selection method</em>.</p> <p><strong>stepwise selection method</strong> Stepwise selection method is a greedy approach to solve an optimization problem. At each stage, it calculates criterion function values of all the states that could be reached in one step. It chooses a path to the best state for each step.</p> <ul> <li>It doesn’t guarantee obtaining the global optimum.</li> <li>But it is the easiest approach to handle(computationally or make an algorithm to solve) optimization problems.</li> <li>It significantly reduces the computational load.</li> <li>And it gives a quite reasonable solution in a limited time.</li> </ul> <p>Here, we could use forward/backward/both stepwise method which has a computational complexity of \(\omicron(p^2)\). Ex) forward method We start from a model with largest T-value \(t^\ast = \frac{b_k}{s(b_k)}\). Then add variable with largest \(PRESS_p\). Repeat this adding process until there is no significant change.</p> <p><u>Remark</u> Generally, computing \(PRESS_p\) is costly. However, when it comes to assessing a linear regression model, we have a good formula to compute this deleted residual \(Y_i - \hat{Y_{i(i)}}\). It is going to be introduced in the next chapter.</p> <h2 id="2-model-validation">2. Model validation</h2> <p>To find a model which performs well in prediction, we check a candidate model against independent data which is unseen, uncorrelated but from the same population. The steps are</p> <ol> <li>divide a given dataset into 2 non-overlapping sets <br>training data : validation data = 7:3 or 6:4</li> <li>make a model using training data \((X_t, Y_t)\) and obtain parameters \(\hat{\beta_t} = (X_t^tX_t)^{-1}X_t^tY_t\)</li> <li>calculate the mean squared prediction error</li> </ol> \[MSPR = \frac{\sum_{i=1}^{n^\ast} (Y_{i,new}-\hat{Y_i})^2}{n^\ast}\] <p>where \(n^\ast\) = number of validation data, and \(\hat{Y_i} = X_{i,new}^t\hat{\beta_t}\). For a properly trained model, \(MSPR &gt; MSE\). But if \(MSPR \gg MSE\), the model is invalid (lack of prediction ability).</p> <hr> <p>Here is the <a href="https://github.com/HyeminGu/Regression_R_tutorials/blob/main/Regression_5_Variable_Selection_in_Multiple_Regression.ipynb" rel="external nofollow noopener" target="_blank">jupyter notebook script</a> to run several practice codes using R.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/SIAM_MDS_talk_w1w2flow/">Wasserstein Proximals Stabilize Training of Generative Models and Learn Manifolds</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/SIAM_MDS_poster_GPA/">Lipschitz-Regularized Gradient Flows and Latent Generative Particles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/ICERM_poster_GPA/">Lipschitz Regularized Gradient Flows and Latent Generative Particles</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/">Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, Γ)-gradient flow</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2021/python_ode_solving/">Python ODE solving tutorial</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Hyemin Gu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-repositories",title:"repositories",description:"Github repositories for coding works.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-wasserstein-proximals-stabilize-training-of-generative-models-and-learn-manifolds",title:"Wasserstein Proximals Stabilize Training of Generative Models and Learn Manifolds",description:"This is a talk slide for SIAM Mathematics of Data Science 2024.",section:"Posts",handler:()=>{window.location.href="/blog/2024/SIAM_MDS_talk_w1w2flow/"}},{id:"post-lipschitz-regularized-gradient-flows-and-latent-generative-particles",title:"Lipschitz-Regularized Gradient Flows and Latent Generative Particles",description:"This is a poster for SIAM Mathematics of Data Science 2024.",section:"Posts",handler:()=>{window.location.href="/blog/2024/SIAM_MDS_poster_GPA/"}},{id:"post-lipschitz-regularized-gradient-flows-and-latent-generative-particles",title:"Lipschitz Regularized Gradient Flows and Latent Generative Particles",description:"This is a poster for Optimal Transport in Data Science - ICERM 2023.",section:"Posts",handler:()=>{window.location.href="/blog/2023/ICERM_poster_GPA/"}},{id:"post-sample-generation-from-unknown-distributions-particle-descent-algorithm-induced-by-f-\u03b3-gradient-flow",title:"Sample generation from unknown distributions - Particle Descent Algorithm induced by (f, \u0393)-gradient...",description:"This is a 2022 Spring Project paper/presentation slide for the Stochastic processes class.",section:"Posts",handler:()=>{window.location.href="/blog/2022/sample_generation_through_gpa_by_gradient_flow/"}},{id:"post-python-ode-solving-tutorial",title:"Python ODE solving tutorial",description:"These are 3 days tutorials(videos, slides, jupyter notebooks) for solving ODEs using Python numerical ODE solvers and introducing PINNs. It was a part of 2021 Fall Nonlinear dynamics class. Conducted by T.A. Hyemin Gu.",section:"Posts",handler:()=>{window.location.href="/blog/2021/python_ode_solving/"}},{id:"post-learning-operators",title:"Learning operators",description:"This is a review presentation slide delivered in machine learning reading seminar in 2021.",section:"Posts",handler:()=>{window.location.href="/blog/2021/learning_operator/"}},{id:"post-lorenz-equations-for-atmospheric-convection-modeling",title:"Lorenz Equations for Atmospheric Convection Modeling",description:"This is a 2021 Spring Project paper for the Applied Math and Math Modeling class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/lorenz_eq/"}},{id:"post-data-dependent-kernel-support-vector-machine-classifiers-in-reproducing-kernel-hilbert-space",title:"Data-dependent Kernel Support Vector Machine classifiers in Reproducing Kernel Hilbert Space",description:"This is a 2021 Spring Project paper for the ST - Math Foundations of Probabilistic Artificial Intelligence II class.",section:"Posts",handler:()=>{window.location.href="/blog/2021/kernelSVM_in_RKHS/"}},{id:"post-r-bioinformatics-2-r-genome-informatics-methodology-and-practice",title:"R Bioinformatics 2. R Genome informatics methodology and practice.",description:"This book is a tutorial for R Bioinformatics practices in 2020 organized by Hyemin Gu and Yijun Kim in Ewha Womans University Mokdong hospital. The tutorial focuses on gene expression data analysis.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_bioinformatics/"}},{id:"post-r-bioinformatics-1-r-statistics",title:"R Bioinformatics 1. R Statistics.",description:"This book is 6 days classes lecture notes for R Bioinformatics class in 2020 taught by Hyemin Gu. The lecture was done by line-by-line code running and its explanation.",section:"Posts",handler:()=>{window.location.href="/blog/2020/r_statistics/"}},{id:"post-ra-ch12-autocorrelation-in-time-series-data",title:"RA ch12 Autocorrelation in time series data",description:"Lecture note chapter 12 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch12/"}},{id:"post-ra-ch11-remedial-measures",title:"RA ch11 Remedial measures",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch11/"}},{id:"post-ra-ch10-diagnostics",title:"RA ch10 Diagnostics",description:"Lecture note chapter 10 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch10/"}},{id:"post-ra-ch9-model-selection-and-validation",title:"RA ch9 Model selection and validation",description:"Lecture note chapter 9 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch9/"}},{id:"post-ra-ch8-regression-models-for-quantitative-and-qualitative-predictors",title:"RA ch8 Regression models for Quantitative and Qualitative predictors",description:"Lecture note chapter 8 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch8/"}},{id:"post-ra-ch7-multiple-regression-2",title:"RA ch7 Multiple Regression 2",description:"Lecture note chapter 7 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch7/"}},{id:"post-ra-ch6-multiple-regression-1",title:"RA ch6 Multiple Regression 1",description:"Lecture note chapter 6 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch6/"}},{id:"post-ra-ch5-matrix-approaches-to-simple-linear-regression",title:"RA ch5 Matrix Approaches to Simple Linear Regression",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch5/"}},{id:"post-ra-ch4-diagnostics-and-remedial-measures",title:"RA ch4 Diagnostics and Remedial Measures",description:"Lecture note chapter 4 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch4/"}},{id:"post-ra-ch3-inferences-in-regression-and-correlation-analysis",title:"RA ch3 Inferences in Regression and Correlation Analysis",description:"Lecture note chapter 3 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch3/"}},{id:"post-ra-ch2-review-of-mathematical-statistics",title:"RA ch2 Review of Mathematical Statistics",description:"Lecture note chapter 2 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch2/"}},{id:"post-ra-ch1-linear-regression-with-one-predictor-variable",title:"RA ch1 Linear Regression with One Predictor Variable",description:"Lecture note chapter 1 for regression analysis taught by Hyemin Gu in 2020",section:"Posts",handler:()=>{window.location.href="/blog/2020/regression_ch1/"}},{id:"post-training-a-2-layer-neural-network-using-svd-generated-weights",title:"Training a 2 layer Neural Network using SVD-generated weights",description:"This is a poster for Joint Mathematics Meetings 2018.",section:"Posts",handler:()=>{window.location.href="/blog/2018/training_nn_from_SVD_weights/"}},{id:"post-necessary-and-sufficient-conditions-for-shortest-vectors-in-lattices-of-low-dimension",title:"Necessary and sufficient conditions for shortest vectors in lattices of low dimension.",description:"This is a poster for Joint Mathematics Meetings 2017.",section:"Posts",handler:()=>{window.location.href="/blog/2017/necessary_and_sufficient_conditions_for_shortest_vectors_in_lattices_of_low_dimension/"}},{id:"news-initiated-a-role-as-a-research-assistant-advised-by-markos-katsoulakis",title:"Initiated a role as a research assistant, advised by Markos Katsoulakis.",description:"",section:"News"},{id:"news-initiated-a-role-as-a-twigs-coordinator",title:"Initiated a role as a TWIGS coordinator.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-passed-oral-exam",title:"Passed oral exam.",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_3/"}},{id:"news-presented-a-poster-at-optimal-transport-in-data-science-icerm-brown-university",title:"Presented a poster at Optimal Transport in Data Science \u2013 ICERM, Brown university....",description:"",section:"News"},{id:"news-paper-for-lipschitz-regularized-generative-particles-algorithm-was-published-at-siam-data-science",title:"Paper for Lipschitz regularized generative particles algorithm was published at SIAM Data Science....",description:"",section:"News"},{id:"news-paper-for-wasserstein-1-wasserstein-2-generative-flow-was-released-at-arxiv",title:"Paper for Wasserstein-1/Wasserstein-2 generative flow was released at Arxiv.",description:"",section:"News"},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%68%67%75@%75%6D%61%73%73.%65%64%75","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"socials-researchgate",title:"ResearchGate",section:"Socials",handler:()=>{window.open("https://www.researchgate.net/profile/Hyemin-Gu-2/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/HyeminGu","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/hyemin-gu-58127a207","_blank")}},{id:"socials-facebook",title:"Facebook",section:"Socials",handler:()=>{window.open("https://facebook.com/hyemin.gu.9022","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>